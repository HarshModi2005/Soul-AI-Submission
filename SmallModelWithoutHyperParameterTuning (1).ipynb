{
  "cells": [
    {
      "source": [
        "# %%\n",
        "\"\"\"\n",
        "Data Download and Conversion to CoNLL Format.\n",
        "\n",
        "This script downloads the CoNLL-2003 dataset from Hugging Face,\n",
        "converts it to CoNLL format, and saves it to the data directory.\n",
        "\n",
        "The script creates three files:\n",
        "    - data/conll2003/eng.train: Training data in CoNLL format.\n",
        "    - data/conll2003/eng.testa: Validation data in CoNLL format.\n",
        "    - data/conll2003/eng.testb: Test data in CoNLL format.\n",
        "\"\"\"\n",
        "# Import necessary libraries\n",
        "import os  # For operating system related tasks like creating directories\n",
        "from datasets import load_dataset  # For loading datasets from Hugging Face\n",
        "\n",
        "# Create directories to store the downloaded and processed data\n",
        "# If the directories already exist, it won't raise an error\n",
        "os.makedirs(\"data/conll2003\", exist_ok=True)\n",
        "\n",
        "# Print a message to indicate the download process\n",
        "print(\"Downloading CoNLL-2003 dataset from HuggingFace...\")\n",
        "\n",
        "# Download the CoNLL-2003 dataset from Hugging Face\n",
        "dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "# Print a message to confirm the download completion\n",
        "print(\"Download complete!\")\n",
        "\n",
        "# Save the downloaded data in CoNLL format\n",
        "print(\"Converting to CoNLL format...\")\n",
        "\n",
        "# Iterate through the different splits of the dataset: train, validation, and test\n",
        "for split in [\"train\", \"validation\", \"test\"]:\n",
        "    # Define the output file path for each split\n",
        "    # The file names are eng.train, eng.testa, and eng.testb for train, validation, and test sets respectively\n",
        "    output_file = f\"data/conll2003/eng.{'train' if split == 'train' else 'testa' if split == 'validation' else 'testb'}\"\n",
        "\n",
        "    # Print a message indicating the current split being processed\n",
        "    print(f\"Processing {split} set -> {output_file}\")\n",
        "\n",
        "    # Open the output file in write mode with UTF-8 encoding\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        # Iterate through each example in the current split\n",
        "        for example in dataset[split]:\n",
        "            # Iterate through each token and its corresponding NER tag\n",
        "            for token, tag in zip(example[\"tokens\"], example[\"ner_tags\"]):\n",
        "                # Convert numeric NER tags to their corresponding string representations\n",
        "                tag_str = \"O\"  # Default tag is \"O\" (Outside)\n",
        "                if tag > 0:  # If the tag is not \"O\"\n",
        "                    # Define a mapping from numeric tags to string tags\n",
        "                    # This mapping is specific to the CoNLL-2003 dataset\n",
        "                    tag_map = {\n",
        "                        1: \"B-PER\", 2: \"I-PER\",  # Person tags\n",
        "                        3: \"B-ORG\", 4: \"I-ORG\",  # Organization tags\n",
        "                        5: \"B-LOC\", 6: \"I-LOC\",  # Location tags\n",
        "                        7: \"B-MISC\", 8: \"I-MISC\"  # Miscellaneous tags\n",
        "                    }\n",
        "                    # Get the string representation of the tag\n",
        "                    tag_str = tag_map[tag]\n",
        "\n",
        "                # Write the token and its tag to the output file\n",
        "                f.write(f\"{token} {tag_str}\\n\")\n",
        "\n",
        "            # Add an empty line to separate sentences\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "# Print a success message\n",
        "print(\"Dataset downloaded and converted to CoNLL format successfully!\")\n",
        "print(\"Files created:\")\n",
        "print(\"  - data/conll2003/eng.train\")\n",
        "print(\"  - data/conll2003/eng.testa\")\n",
        "print(\"  - data/conll2003/eng.testb\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "S0N6_NIz6j36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# %%\n",
        "\"\"\"\n",
        "Data Preprocessing and Exploration.\n",
        "\n",
        "This script reads the CoNLL-2003 dataset, performs data exploration,\n",
        "and preprocesses the data for different NER models.\n",
        "\n",
        "The script creates preprocessed data in various formats:\n",
        "    - SpaCy format: Data formatted for use with SpaCy NER models.\n",
        "    - BIO format: Data in BIO (Beginning, Inside, Outside) tagging format.\n",
        "    - JSON format: Data stored in JSON format for easy access and manipulation.\n",
        "    - Transformer format: Data preprocessed for use with transformer-based NER models.\n",
        "\"\"\"\n",
        "# Import necessary libraries\n",
        "import os  # For operating system related tasks like creating directories\n",
        "import matplotlib.pyplot as plt  # For creating visualizations\n",
        "import seaborn as sns  # For creating statistical visualizations\n",
        "import nltk  # For natural language processing tasks\n",
        "from nltk.corpus import stopwords  # For removing common words\n",
        "from nltk.stem import WordNetLemmatizer  # For reducing words to their base form\n",
        "import pickle  # For saving and loading Python objects\n",
        "import json  # For working with JSON data\n",
        "import sys  # For system-specific parameters and functions\n",
        "import subprocess  # For running external commands\n",
        "import spacy  # For advanced natural language processing\n",
        "from collections import Counter  # For counting the frequency of items\n",
        "from typing import List, Dict, Tuple, Optional  # For type hinting\n",
        "\n",
        "# Download NLTK resources needed for preprocessing\n",
        "nltk.download('punkt', quiet=True)  # For sentence tokenization\n",
        "nltk.download('punkt_tab', quiet=True)  # For tokenization with tab separation\n",
        "nltk.download('stopwords', quiet=True)  # For removing common words\n",
        "nltk.download('wordnet', quiet=True)  # For lemmatization\n",
        "\n",
        "# Function to check and install the SpaCy model if not already installed\n",
        "def ensure_spacy_model(model_name=\"en_core_web_sm\"):\n",
        "    \"\"\"\n",
        "    Ensure that the required SpaCy model is installed.\n",
        "\n",
        "    Args:\n",
        "        model_name: The name of the SpaCy model to check and install.\n",
        "            Defaults to \"en_core_web_sm\".\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Attempt to load the SpaCy model\n",
        "        spacy.load(model_name)\n",
        "        # If successful, print a message indicating it's already installed\n",
        "        print(f\"SpaCy model '{model_name}' is already installed.\")\n",
        "    except OSError:\n",
        "        # If loading fails, print a message indicating it's not found\n",
        "        print(f\"SpaCy model '{model_name}' not found. Installing...\")\n",
        "        # Install the SpaCy model using subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", model_name])\n",
        "        # Print a message indicating the installation is complete\n",
        "        print(f\"SpaCy model '{model_name}' has been installed.\")\n",
        "\n",
        "# Ensure the SpaCy model is installed\n",
        "ensure_spacy_model()\n",
        "\n",
        "# Load the SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to read a CoNLL formatted file and extract sentences with their tags\n",
        "def read_conll_file(file_path: str) -> List[List[Tuple[str, str]]]:\n",
        "    \"\"\"\n",
        "    Read a CoNLL-2003 formatted file and return sentences with their tags.\n",
        "\n",
        "    Args:\n",
        "        file_path: The path to the CoNLL formatted file.\n",
        "\n",
        "    Returns:\n",
        "        A list of sentences, where each sentence is a list of (word, tag) tuples.\n",
        "    \"\"\"\n",
        "    sentences = []  # Initialize an empty list to store sentences\n",
        "    current_sentence = []  # Initialize an empty list to store the current sentence\n",
        "\n",
        "    # Open the file in read mode with UTF-8 encoding\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        # Iterate through each line in the file\n",
        "        for line in f:\n",
        "            line = line.strip()  # Remove leading/trailing whitespace\n",
        "\n",
        "            # Skip empty lines, comments, and metadata lines\n",
        "            if not line or line.startswith('-DOCSTART-') or line.startswith('//'):\n",
        "                # If the current sentence is not empty, add it to the list of sentences\n",
        "                if current_sentence:\n",
        "                    sentences.append(current_sentence)\n",
        "                    current_sentence = []  # Reset the current sentence\n",
        "                continue  # Move to the next line\n",
        "\n",
        "            # Split the line by whitespace to get the word and tag\n",
        "            parts = line.split()\n",
        "            # Ensure there's at least a word and a tag\n",
        "            if len(parts) >= 2:\n",
        "                word = parts[0]  # The word is the first element\n",
        "                tag = parts[-1]  # The tag is the last element\n",
        "                current_sentence.append((word, tag))  # Add the (word, tag) tuple to the current sentence\n",
        "\n",
        "    # Add the last sentence if it's not empty\n",
        "    if current_sentence:\n",
        "        sentences.append(current_sentence)\n",
        "\n",
        "    # Return the list of sentences\n",
        "    return sentences\n",
        "\n",
        "# Function to explore the dataset and extract statistics\n",
        "def explore_dataset(sentences: List[List[Tuple[str, str]]]) -> Dict:\n",
        "    \"\"\"\n",
        "    Explore the dataset and return statistics.\n",
        "\n",
        "    Args:\n",
        "        sentences: A list of sentences, where each sentence is a list of (word, tag) tuples.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing various statistics about the dataset.\n",
        "    \"\"\"\n",
        "    # Calculate the number of sentences\n",
        "    num_sentences = len(sentences)\n",
        "\n",
        "    # Calculate the total number of words\n",
        "    total_words = sum(len(sentence) for sentence in sentences)\n",
        "\n",
        "    # Calculate the number of unique words\n",
        "    unique_words = set()  # Use a set to store unique words\n",
        "    for sentence in sentences:\n",
        "        for word, _ in sentence:\n",
        "            unique_words.add(word.lower())  # Add words in lowercase to the set\n",
        "    num_unique_words = len(unique_words)\n",
        "\n",
        "    # Count entity types and their occurrences\n",
        "    entity_counts = Counter()  # Use a Counter to count entity types\n",
        "    entity_length_distribution = {}  # Store entity length distributions\n",
        "    current_entity = None  # Track the current entity being processed\n",
        "    current_entity_length = 0  # Track the length of the current entity\n",
        "\n",
        "    # Iterate through sentences and tokens to count entities and their lengths\n",
        "    for sentence in sentences:\n",
        "        for _, tag in sentence:\n",
        "            if tag.startswith('B-'):  # Check for the beginning of an entity\n",
        "                # If we were tracking an entity, finalize it\n",
        "                if current_entity:\n",
        "                    entity_length_distribution.setdefault(current_entity, []).append(current_entity_length)\n",
        "\n",
        "                # Start tracking a new entity\n",
        "                current_entity = tag[2:]  # Extract the entity type (remove 'B-')\n",
        "                current_entity_length = 1  # Initialize the entity length\n",
        "                entity_counts[current_entity] += 1  # Increment the entity count\n",
        "\n",
        "            elif tag.startswith('I-'):  # Check for continuation of an entity\n",
        "                # Continue the current entity if it matches\n",
        "                if current_entity == tag[2:]:\n",
        "                    current_entity_length += 1  # Increment the entity length\n",
        "\n",
        "            else:  # 'O' tag (Outside)\n",
        "                # If we were tracking an entity, finalize it\n",
        "                if current_entity:\n",
        "                    entity_length_distribution.setdefault(current_entity, []).append(current_entity_length)\n",
        "                    current_entity = None  # Reset the current entity\n",
        "                    current_entity_length = 0  # Reset the entity length\n",
        "\n",
        "    # Calculate average entity length\n",
        "    avg_entity_length = {}\n",
        "    for entity, lengths in entity_length_distribution.items():\n",
        "        # Calculate average length if there are lengths for the entity\n",
        "        avg_entity_length[entity] = sum(lengths) / len(lengths) if lengths else 0\n",
        "\n",
        "    # Calculate sentence length statistics\n",
        "    sentence_lengths = [len(sentence) for sentence in sentences]  # Get lengths of all sentences\n",
        "    # Calculate average sentence length\n",
        "    avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0\n",
        "    # Calculate maximum sentence length\n",
        "    max_sentence_length = max(sentence_lengths) if sentence_lengths else 0\n",
        "\n",
        "    # Return the collected statistics in a dictionary\n",
        "    return {\n",
        "        'num_sentences': num_sentences,\n",
        "        'total_words': total_words,\n",
        "        'num_unique_words': num_unique_words,\n",
        "        'entity_counts': dict(entity_counts),\n",
        "        'avg_entity_length': avg_entity_length,\n",
        "        'avg_sentence_length': avg_sentence_length,\n",
        "        'max_sentence_length': max_sentence_length\n",
        "    }\n",
        "\n",
        "# Function to visualize the distribution of entity types\n",
        "def visualize_entity_distribution(stats: Dict, save_path: Optional[str] = None):\n",
        "    \"\"\"\n",
        "    Visualize the distribution of entity types using a bar chart.\n",
        "\n",
        "    Args:\n",
        "        stats: A dictionary containing dataset statistics, including entity counts.\n",
        "        save_path: The path to save the visualization (optional). If None,\n",
        "            the plot will be displayed instead of saved.\n",
        "    \"\"\"\n",
        "    # Create a figure and axes for the plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Extract entity counts from the statistics dictionary\n",
        "    entity_counts = stats['entity_counts']\n",
        "    entities = list(entity_counts.keys())  # Get the entity types\n",
        "    counts = list(entity_counts.values())  # Get the corresponding counts\n",
        "\n",
        "    # Create a bar chart\n",
        "    plt.bar(entities, counts)\n",
        "\n",
        "    # Set the title and labels for the plot\n",
        "    plt.title('Distribution of Entity Types')\n",
        "    plt.xlabel('Entity Type')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    # Rotate x-axis labels for better readability\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Save or display the plot based on the save_path argument\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')  # Save the plot to a file\n",
        "    else:\n",
        "        plt.show()  # Display the plot\n",
        "\n",
        "    # Close the plot to release resources\n",
        "    plt.close()\n",
        "\n",
        "# Function to preprocess text using NLTK\n",
        "def preprocess_text(text: str, remove_stopwords: bool = True, lemmatize: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Preprocess text using NLTK for lowercasing, stopword removal, and lemmatization.\n",
        "\n",
        "    Args:\n",
        "        text: The input text to preprocess.\n",
        "        remove_stopwords: Whether to remove stopwords (default: True).\n",
        "        lemmatize: Whether to lemmatize words (default: True).\n",
        "\n",
        "    Returns:\n",
        "        The preprocessed text.\n",
        "    \"\"\"\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords if requested\n",
        "    if remove_stopwords:\n",
        "        stop_words = set(stopwords.words('english'))  # Get a set of English stopwords\n",
        "        tokens = [token for token in tokens if token not in stop_words]  # Filter out stopwords\n",
        "\n",
        "    # Lemmatize words if requested\n",
        "    if lemmatize:\n",
        "        lemmatizer = WordNetLemmatizer()  # Create a lemmatizer object\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize each token\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Function to preprocess text using SpaCy\n",
        "def preprocess_spacy(text: str, remove_stopwords: bool = True, lemmatize: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Preprocess text using SpaCy for better tokenization and lemmatization.\n",
        "\n",
        "    Args:\n",
        "        text: The input text to preprocess.\n",
        "        remove_stopwords: Whether to remove stopwords (default: True).\n",
        "        lemmatize: Whether to lemmatize words (default: True).\n",
        "\n",
        "    Returns:\n",
        "        The preprocessed text.\n",
        "    \"\"\"\n",
        "    # Process the text using the loaded SpaCy model\n",
        "    doc = nlp(text)\n",
        "\n",
        "    tokens = []  # Initialize a list to store processed tokens\n",
        "    # Iterate through each token in the SpaCy document\n",
        "    for token in doc:\n",
        "        # Skip stopwords if requested\n",
        "        if remove_stopwords and token.is_stop:\n",
        "            continue\n",
        "\n",
        "        # Use lemma if requested, otherwise use the original token text\n",
        "        processed_token = token.lemma_ if lemmatize else token.text\n",
        "        # Lowercase the token\n",
        "        processed_token = processed_token.lower()\n",
        "\n",
        "        # Add the processed token to the list\n",
        "        tokens.append(processed_token)\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Function to convert CoNLL data to SpaCy format\n",
        "def convert_to_spacy_format(sentences: List[List[Tuple[str, str]]], output_file: str):\n",
        "    \"\"\"\n",
        "    Convert CoNLL data to spaCy format and save it to a pickle file.\n",
        "\n",
        "    Args:\n",
        "        sentences: A list of sentences, where each sentence is a list of (word, tag) tuples.\n",
        "        output_file: The path to the output pickle file.\n",
        "    \"\"\"\n",
        "    training_data = []  # Initialize a list to store training data in SpaCy format\n",
        "\n",
        "    # Iterate through each sentence in the CoNLL data\n",
        "    for sentence in sentences:\n",
        "        words = [word for word, _ in sentence]  # Extract words from the sentence\n",
        "        tags = [tag for _, tag in sentence]  # Extract tags from the sentence\n",
        "\n",
        "        text = ' '.join(words)  # Join words to form the sentence text\n",
        "        entities = []  # Initialize a list to store entity information\n",
        "\n",
        "        # Extract entity spans and their types\n",
        "        i = 0\n",
        "        while i < len(tags):\n",
        "            if tags[i].startswith('B-'):  # Check for the beginning of an entity\n",
        "                entity_type = tags[i][2:]  # Extract the entity type\n",
        "                start = i  # Store the start index of the entity\n",
        "                end = i + 1  # Initialize the end index of the entity\n",
        "\n",
        "                # Find the end of the entity\n",
        "                while end < len(tags) and tags[end].startswith('I-') and tags[end][2:] == entity_type:\n",
        "                    end += 1  # Extend the end index if the entity continues\n",
        "\n",
        "                # Calculate character spans for the entity\n",
        "                char_start = len(' '.join(words[:start]))\n",
        "                if start > 0:\n",
        "                    char_start += 1  # Add 1 for the space before the entity\n",
        "                char_end = char_start + len(' '.join(words[start:end]))\n",
        "\n",
        "                # Add the entity information (start, end, type) to the list\n",
        "                entities.append((char_start, char_end, entity_type))\n",
        "                i = end  # Move the index to the next token after the entity\n",
        "            else:\n",
        "                i += 1  # Move to the next token\n",
        "\n",
        "        # Add the sentence text and entity information to the training data\n",
        "        training_data.append((text, {'entities': entities}))\n",
        "\n",
        "    # Save the training data to a pickle file\n",
        "    with open(output_file, 'wb') as f:\n",
        "        pickle.dump(training_data, f)\n",
        "\n",
        "    # Return the training data\n",
        "    return training_data\n",
        "\n",
        "# Function to convert data to BIO format\n",
        "def convert_to_bio_format(sentences: List[List[Tuple[str, str]]], output_file: str):\n",
        "    \"\"\"\n",
        "    Convert data to BIO (Beginning, Inside, Outside) format and save it to a file.\n",
        "\n",
        "    Args:\n",
        "        sentences: A list of sentences, where each sentence is a list of (word, tag) tuples.\n",
        "        output_file: The path to the output file.\n",
        "    \"\"\"\n",
        "    # Open the output file in write mode with UTF-8 encoding\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        # Iterate through each sentence in the data\n",
        "        for sentence in sentences:\n",
        "            # Iterate through each word and tag in the sentence\n",
        "            for word, tag in sentence:\n",
        "                # Write the word and tag to the file in BIO format\n",
        "                f.write(f\"{word} {tag}\\n\")\n",
        "            # Add an empty line to separate sentences\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "# Function to convert data to JSON format\n",
        "def convert_to_json_format(sentences: List[List[Tuple[str, str]]], output_file: str):\n",
        "    \"\"\"\n",
        "    Convert data to JSON format and save it to a file.\n",
        "\n",
        "    Args:\n",
        "        sentences: A list of sentences, where each sentence is a list of (word, tag) tuples.\n",
        "        output_file: The path to the output JSON file.\n",
        "    \"\"\"\n",
        "    data = []  # Initialize a list to store the data in JSON format\n",
        "\n",
        "    # Iterate through each sentence in the data\n",
        "    for sentence in sentences:\n",
        "        words = [word for word, _ in sentence]  # Extract words from the sentence\n",
        "        tags = [tag for _, tag in sentence]  # Extract tags from the sentence\n",
        "\n",
        "        # Create a dictionary representing the sentence with words, tokens, and tags\n",
        "        data.append({\n",
        "            'text': ' '.join(words),  # Join words to form the sentence text\n",
        "            'tokens': words,  # List of words in the sentence\n",
        "            'tags': tags  # List of tags corresponding to the words\n",
        "        })\n",
        "\n",
        "    # Open the output file in write mode with UTF-8 encoding\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        # Write the data to the JSON file with indentation for readability\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "# Function to preprocess data for transformer models\n",
        "def preprocess_data_for_transformers(sentences: List[List[Tuple[str, str]]]):\n",
        "    \"\"\"\n",
        "    Preprocess data specifically for transformer models like BERT.\n",
        "\n",
        "    Args:\n",
        "        sentences: A list of sentences, where each sentence is a list of (word, tag) tuples.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing two lists:\n",
        "            - tokenized_texts: A list of tokenized sentences (list of words).\n",
        "            - tags_list: A list of corresponding tags for each sentence.\n",
        "    \"\"\"\n",
        "    tokenized_texts = []  # Initialize a list to store tokenized texts\n",
        "    tags_list = []  # Initialize a list to store corresponding tags\n",
        "\n",
        "    # Iterate through each sentence in the data\n",
        "    for sentence in sentences:\n",
        "        words = [word for word, _ in sentence]  # Extract words from the sentence\n",
        "        tags = [tag for _, tag in sentence]  # Extract tags from the sentence\n",
        "\n",
        "        # Append the words and tags to their respective lists\n",
        "        tokenized_texts.append(words)\n",
        "        tags_list.append(tags)\n",
        "\n",
        "    # Return the tokenized texts and tags\n",
        "    return tokenized_texts, tags_list\n",
        "\n",
        "# Function to create a mapping of entity labels to IDs\n",
        "def create_entity_labels_mapping(sentences: List[List[Tuple[str, str]]]):\n",
        "    \"\"\"\n",
        "    Create a mapping of entity labels to numerical IDs.\n",
        "\n",
        "    Args:\n",
        "        sentences: A list of sentences, where each sentence is a list of (word, tag) tuples.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing two dictionaries:\n",
        "            - tag_to_id: A dictionary mapping entity labels (tags) to numerical IDs.\n",
        "            - id_to_tag: A dictionary mapping numerical IDs back to entity labels (tags).\n",
        "    \"\"\"\n",
        "    unique_tags = set()  # Use a set to store unique entity labels\n",
        "\n",
        "    # Iterate through sentences and tokens to collect unique tags\n",
        "    for sentence in sentences:\n",
        "        for _, tag in sentence:\n",
        "            unique_tags.add(tag)\n",
        "\n",
        "    # Sort the unique tags and create mappings\n",
        "    # tag_to_id maps each tag to a unique numerical ID\n",
        "    tag_to_id = {tag: i for i, tag in enumerate(sorted(list(unique_tags)))}\n",
        "    # id_to_tag maps each numerical ID back to its corresponding tag\n",
        "    id_to_tag = {i: tag for tag, i in tag_to_id.items()}\n",
        "\n",
        "    # Return the tag-to-ID and ID-to-tag mappings\n",
        "    return tag_to_id, id_to_tag\n",
        "\n",
        "# Main function to execute the preprocessing steps\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to demonstrate data preprocessing functions.\n",
        "    \"\"\"\n",
        "    # Define the data directory\n",
        "    data_dir = \"/content/"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Gk0llymJ6pRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# %%\n",
        "\"\"\"\n",
        "Training Script for BERT-based NER Model.\n",
        "\n",
        "This script trains a BERT-based Named Entity Recognition (NER) model\n",
        "using the preprocessed CoNLL-2003 dataset. It leverages the Hugging Face\n",
        "Transformers library for BERT and PyTorch for model training.\n",
        "\n",
        "The script follows these key steps:\n",
        "1. Data Loading: Loads preprocessed data and entity tag mappings.\n",
        "2. Data Encoding: Encodes the dataset using the BERT tokenizer,\n",
        "   handling subword tokenization and aligning tags.\n",
        "3. Data Loading and Batching: Creates PyTorch DataLoaders for\n",
        "   efficient training and validation data handling.\n",
        "4. Model Initialization: Initializes the BERT model for token\n",
        "   classification with the appropriate number of entity labels.\n",
        "5. Training Setup: Defines the optimizer, learning rate scheduler,\n",
        "   and loss function for model training.\n",
        "6. Training Loop: Trains the model for a specified number of epochs,\n",
        "   iterating over batches of data and updating model parameters.\n",
        "7. Validation: Evaluates the model's performance on the validation\n",
        "   set after each epoch, calculating metrics like precision, recall,\n",
        "   F1-score, and accuracy.\n",
        "8. Model Saving: Saves the trained model and tokenizer for later use.\n",
        "\"\"\"\n",
        "\n",
        "# Import necessary libraries and modules\n",
        "import os  # For operating system interactions (e.g., creating directories)\n",
        "import json  # For working with JSON data (e.g., loading tag mappings)\n",
        "import pickle  # For object serialization (e.g., loading preprocessed data)\n",
        "import numpy as np  # For numerical operations (e.g., array manipulation)\n",
        "import torch  # For deep learning operations (e.g., tensors, model training)\n",
        "from torch.utils.data import DataLoader, TensorDataset  # For data loading and batching\n",
        "from torch.optim import AdamW  # For optimization (AdamW optimizer)\n",
        "from torch.nn import CrossEntropyLoss  # For calculating loss (cross-entropy)\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score  # For evaluation metrics\n",
        "from transformers import (  # For using pre-trained transformer models\n",
        "    BertTokenizer,  # For tokenizing text with BERT\n",
        "    BertForTokenClassification,  # For token classification tasks\n",
        "    get_linear_schedule_with_warmup  # For learning rate scheduling\n",
        ")\n",
        "from typing import Dict, List, Tuple  # For type hinting (improved code readability)\n",
        "from tqdm import tqdm  # For progress bars (visualizing training progress)\n",
        "import matplotlib.pyplot as plt  # For creating visualizations (e.g., training curves)\n",
        "import seaborn as sns  # For creating statistical visualizations (e.g., data distributions)\n",
        "\n",
        "# Function to load preprocessed data and entity tag mappings\n",
        "def load_preprocessed_data(data_dir: str) -> Tuple[Dict, Dict]:\n",
        "    \"\"\"\n",
        "    Loads preprocessed data for transformer models and entity tag mappings.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): The directory containing the preprocessed data.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict, Dict]: A tuple containing the transformer data and tag mappings.\n",
        "    \"\"\"\n",
        "    # Construct the path to the transformer data directory\n",
        "    transformer_dir = os.path.join(data_dir, \"processed\", \"transformer\")\n",
        "\n",
        "    # Load the transformer data from the pickle file\n",
        "    with open(os.path.join(transformer_dir, \"transformer_data.pickle\"), \"rb\") as f:\n",
        "        transformer_data = pickle.load(f)\n",
        "\n",
        "    # Load the tag mappings from the JSON file\n",
        "    with open(os.path.join(transformer_dir, \"tag_mappings.json\"), \"r\") as f:\n",
        "        tag_mappings = json.load(f)\n",
        "\n",
        "    # Return the transformer data and tag mappings\n",
        "    return transformer_data, tag_mappings\n",
        "\n",
        "# Function to encode the dataset using the BERT tokenizer\n",
        "def encode_dataset(texts: List[List[str]], tags: List[List[str]],\n",
        "                  tokenizer, tag_to_id: Dict, max_length: int = 128) -> Tuple:\n",
        "    \"\"\"\n",
        "    Encodes the dataset using the BERT tokenizer, handling subword tokenization.\n",
        "\n",
        "    Args:\n",
        "        texts (List[List[str]]): List of tokenized sentences.\n",
        "        tags (List[List[str]]): List of corresponding tags for each sentence.\n",
        "        tokenizer: BERT tokenizer instance.\n",
        "        tag_to_id (Dict): Mapping of entity labels to IDs.\n",
        "        max_length (int, optional): Maximum sequence length for padding/truncation.\n",
        "                                    Defaults to 128.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: A tuple containing encoded input IDs, attention masks, and tag IDs.\n",
        "    \"\"\"\n",
        "    # Initialize lists to store encoded data\n",
        "    input_ids = []  # Store input IDs for BERT\n",
        "    attention_masks = []  # Store attention masks for BERT\n",
        "    tag_ids = []  # Store encoded tag IDs\n",
        "\n",
        "    # Iterate through each sentence and its corresponding tags\n",
        "    for sentence, sentence_tags in zip(texts, tags):\n",
        "        # Tokenize the sentence using the BERT tokenizer\n",
        "        encoded = tokenizer(\n",
        "            sentence,\n",
        "            is_split_into_words=True,  # Input is already tokenized\n",
        "            add_special_tokens=True,  # Add [CLS] and [SEP] tokens\n",
        "            max_length=max_length,  # Pad/truncate to max_length\n",
        "            padding=\"max_length\",  # Pad to max_length\n",
        "            truncation=True,  # Truncate if longer than max_length\n",
        "            return_attention_mask=True  # Return attention mask\n",
        "        )\n",
        "\n",
        "        # Get tokens, input IDs, and attention mask\n",
        "        tokens = encoded.tokens()  # Get tokens from tokenizer output\n",
        "        ids = encoded[\"input_ids\"]  # Get input IDs\n",
        "        mask = encoded[\"attention_mask\"]  # Get attention mask\n",
        "\n",
        "        # Align tags with subword tokens\n",
        "        aligned_tags = []  # Store aligned tags\n",
        "        current_word_idx = 0  # Track current word index in original sentence\n",
        "\n",
        "        # Iterate over subword tokens\n",
        "        for token_idx, token in enumerate(tokens):\n",
        "            # Skip special tokens ([CLS], [SEP], [PAD])\n",
        "            if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
        "                aligned_tags.append(\"O\")  # Assign \"O\" tag to special tokens\n",
        "                continue\n",
        "\n",
        "            # Check if token is a subword (starts with '##')\n",
        "            if token.startswith(\"##\"):\n",
        "                # If subword, assign the same tag as the previous word\n",
        "                aligned_tags.append(aligned_tags[-1])\n",
        "            else:\n",
        "                # If not a subword, assign the corresponding tag from the original sentence\n",
        "                aligned_tags.append(sentence_tags[current_word_idx])\n",
        "                # Move to the next word in the original sentence\n",
        "                current_word_idx += 1\n",
        "\n",
        "        # Convert tags to numerical IDs using the tag_to_id mapping\n",
        "        aligned_tag_ids = [tag_to_id[tag] for tag in aligned_tags]\n",
        "\n",
        "        # Append encoded data to lists\n",
        "        input_ids.append(ids)  # Append input IDs to the list\n",
        "        attention_masks.append(mask)  # Append attention mask to the list\n",
        "        tag_ids.append(aligned_tag_ids)  # Append aligned tag IDs to the list\n",
        "\n",
        "    # Convert lists to PyTorch tensors\n",
        "    input_ids = torch.tensor(input_ids)  # Convert input IDs to tensor\n",
        "    attention_masks = torch.tensor(attention_masks)  # Convert attention masks to tensor\n",
        "    tag_ids = torch.tensor(tag_ids)  # Convert tag IDs to tensor\n",
        "\n",
        "    # Return encoded data as tensors\n",
        "    return input_ids, attention_masks, tag_ids  # Return encoded data\n",
        "\n",
        "# Function to calculate metrics\n",
        "def calculate_metrics(true_tags: List[List[str]], pred_tags: List[List[str]]) -> Dict:\n",
        "    \"\"\"\n",
        "    Calculates precision, recall, F1-score, and accuracy.\n",
        "\n",
        "    Args:\n",
        "        true_tags (List[List[str]]): List of true tags for each sentence.\n",
        "        pred_tags (List[List[str]]): List of predicted tags for each sentence.\n",
        "\n",
        "    Returns:\n",
        "        Dict: A dictionary containing precision, recall, F1-score, and accuracy.\n",
        "    \"\"\"\n",
        "    # Flatten the lists for metric calculation\n",
        "    flat_true_tags = [tag for sublist in true_tags for tag in sublist]  # Flatten true tags\n",
        "    flat_pred_tags = [tag for sublist in pred_tags for tag in sublist]  # Flatten predicted tags\n",
        "\n",
        "    # Calculate precision, recall, and F1-score using sklearn's metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        flat_true_tags, flat_pred_tags, average=\"weighted\", zero_division=0  # Handle zero division\n",
        "    )\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(flat_true_tags, flat_pred_tags)  # Calculate accuracy\n",
        "\n",
        "    # Return the calculated metrics as a dictionary\n",
        "    return {\n",
        "        \"precision\": precision,  # Precision score\n",
        "        \"recall\": recall,  # Recall score\n",
        "        \"f1\": f1,  # F1 score\n",
        "        \"accuracy\": accuracy  # Accuracy score\n",
        "    }\n",
        "\n",
        "# Function to train the BERT model\n",
        "def train_model(data_dir: str, model_name: str = \"bert-base-uncased\",\n",
        "              epochs: int = 3, batch_size: int = 32, learning_rate: float = 2e-5,\n",
        "              max_length: int = 128) -> None:\n",
        "    \"\"\"\n",
        "    Trains a BERT-based NER model.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Directory containing the preprocessed data.\n",
        "        model_name (str, optional): Name of the pre-trained BERT model to use.\n",
        "                                     Defaults to \"bert-base-uncased\".\n",
        "        epochs (int, optional): Number of training epochs. Defaults to 3.\n",
        "        batch_size (int, optional): Batch size for training. Defaults to 32.\n",
        "        learning_rate (float, optional): Learning rate for the optimizer.\n",
        "                                         Defaults to 2e-5.\n",
        "        max_length (int, optional): Maximum sequence length for padding/truncation.\n",
        "                                    Defaults to 128.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load preprocessed data and tag mappings\n",
        "    transformer_data, tag_mappings = load_preprocessed_data(data_dir)  # Load data and mappings\n",
        "    tag_to_id = tag_mappings[\"tag_to_id\"]  # Get tag-to-ID mapping\n",
        "    id_to_tag = tag_mappings[\"id_to_tag\"]  # Get ID-to-tag mapping\n",
        "\n",
        "    # Initialize BERT tokenizer and model\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)  # Initialize tokenizer\n",
        "    model = BertForTokenClassification.from_pretrained(\n",
        "        model_name, num_labels=len(tag_to_id)  # Initialize model with num_labels\n",
        "    )\n",
        "\n",
        "    # Encode datasets\n",
        "    train_inputs, train_masks, train_tags = encode_dataset(\n",
        "        transformer_data[\"train\"][\"texts\"],  # Training texts\n",
        "        transformer_data[\"train\"][\"tags\"],  # Training tags\n",
        "        tokenizer, tag_to_id, max_length  # Tokenizer, tag mapping, max length\n",
        "    )\n",
        "    dev_inputs, dev_masks, dev_tags = encode_dataset(\n",
        "        transformer_data[\"dev\"][\"texts\"],  # Validation texts\n",
        "        transformer_data[\"dev\"][\"tags\"],  # Validation tags\n",
        "        tokenizer, tag_to_id, max_length  # Tokenizer, tag mapping, max length\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataset = TensorDataset(train_inputs, train_masks, train_tags)  # Create training dataset\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Create training loader\n",
        "    dev_dataset = TensorDataset(dev_inputs, dev_masks, dev_tags)  # Create validation dataset\n",
        "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)  # Create validation loader\n",
        "\n",
        "    # Define optimizer, scheduler, and loss function\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)  # Initialize AdamW optimizer\n",
        "    total_steps = len(train_loader) * epochs  # Calculate total training steps\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=0, num_training_steps=total_steps  # Initialize scheduler\n",
        "    )\n",
        "    loss_fn = CrossEntropyLoss()  # Initialize cross-entropy loss function\n",
        "\n",
        "    # Training loop\n",
        "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Move model to device (GPU if available)\n",
        "    device = model.device  # Get device of the model\n",
        "\n",
        "    # Iterate over epochs\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        total_loss = 0  # Initialize total loss for the epoch\n",
        "\n",
        "        # Iterate over batches in the training data loader using tqdm for progress bar\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
        "            # Unpack batch data and move to device\n",
        "            inputs, masks, tags = batch  # Unpack batch data\n",
        "            inputs = inputs.to(device)  # Move inputs to device\n",
        "            masks = masks.to(device)  # Move masks to device\n",
        "            tags = tags.to(device)  # Move tags to device\n",
        "\n",
        "            # Zero out gradients from previous step\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "            # Perform forward pass\n",
        "            outputs = model(inputs, attention_mask=masks)  # Get model outputs\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_fn(outputs.logits.view(-1, outputs.logits.shape[-1]), tags.view(-1))  # Calculate loss\n",
        "\n",
        "            # Perform backward pass and update model parameters\n",
        "            loss.backward()  # Calculate gradients\n",
        "            optimizer.step()  # Update model parameters\n",
        "            scheduler.step()  # Update learning rate\n",
        "\n",
        "            # Accumulate total loss for the epoch\n",
        "            total_loss += loss.item()  # Add batch loss to total loss\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = total_loss / len(train_loader)  # Calculate average loss\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss:.4f}\")  # Print epoch loss\n",
        "\n",
        "        # Evaluation on validation set\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        all_pred_tags = []  # Store all predicted tags\n",
        "        all_true_tags = []  # Store all true tags\n",
        "\n",
        "        # Disable gradient calculation during evaluation\n",
        "        with torch.no_grad():\n",
        "            # Iterate over batches in the validation data loader\n",
        "            for batch in dev_loader:\n",
        "                # Unpack batch data and move to device\n",
        "                inputs, masks, tags = batch  # Unpack batch data\n",
        "                inputs = inputs.to(device)  # Move inputs to device\n",
        "                masks = masks.to(device)  # Move masks to device\n",
        "                tags = tags.to(device)  # Move tags to device\n",
        "\n",
        "                # Perform forward pass\n",
        "                outputs = model(inputs, attention_mask=masks)  # Get model outputs\n",
        "\n",
        "                # Get predicted tags\n",
        "                predictions = torch.argmax(outputs.logits, dim=2)  # Get predicted tag IDs\n",
        "\n",
        "                # Convert predicted and true tags to original format\n",
        "                for i in range(inputs.shape[0]):\n",
        "                    # Extract predicted tags for the current sentence\n",
        "                    pred_tags = [id_to_tag[str(tag_id.item())]  # Convert tag ID to tag\n",
        "                                  for tag_id in predictions[i][masks[i] == 1]  # Iterate over valid tokens\n",
        "                                  if tag_id != tag_to_id['[PAD]']]  # Exclude padding tokens\n",
        "                    # Extract true tags for the current sentence\n",
        "                    true_tags = [id_to_tag[str(tag_id.item())]  # Convert tag ID to tag\n",
        "                                 for tag_id in tags[i][masks[i] == 1]  # Iterate over valid tokens\n",
        "                                 if tag_id != tag_to_id['[PAD]']]  # Exclude padding tokens\n",
        "\n",
        "                    # Append predicted and true tags to the lists\n",
        "                    all_pred_tags.append(pred_tags)  # Append predicted tags\n",
        "                    all_true_tags.append(true_tags)  # Append true tags\n",
        "\n",
        "        # Calculate and print evaluation metrics\n",
        "        metrics = calculate_metrics(all_true_tags, all_pred_tags)  # Calculate metrics\n",
        "        print(f\"Validation Metrics: {metrics}\")  # Print validation metrics\n",
        "\n",
        "    # Save the trained model and tokenizer\n",
        "    os.makedirs(\"models\", exist_ok=True)  # Create 'models' directory if it doesn't exist\n",
        "    model.save_pretrained(\"models/bert_ner\")  # Save the trained model\n",
        "    tokenizer.save_pretrained(\"models/bert_ner\")  # Save the tokenizer\n",
        "    print(\"Model and tokenizer saved to 'models/bert_ner'\")  # Print save location\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    # Set the data directory\n",
        "    data_dir = \"data\"  # Define data directory\n",
        "\n",
        "    # Train the model\n",
        "    train_model(data_dir)  # Call the train_model function to start training"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "VErZYF8uD2ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZD5kYTvsZUk",
        "outputId": "a9919eff-3f91-4ce1-d3bd-ea5b67d60cea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=4898eadf8d4badda107c0c8d20f043be6d4ef0664b1c3be10e48aae8615e4abb\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyHKmBvDspt2",
        "outputId": "e9f3b52d-35f7-43e3-a613-776a5dbefce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Using 2 dataloader workers\n",
            "Training set: 5000 examples\n",
            "Validation set: 3250 examples\n",
            "Test set: 3453 examples\n",
            "Encoding datasets...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding dataset: 100%|██████████| 5000/5000 [00:08<00:00, 599.35it/s]\n",
            "Encoding dataset: 100%|██████████| 3250/3250 [00:07<00:00, 461.18it/s]\n",
            "Encoding dataset:  31%|███       | 1060/3453 [00:00<00:01, 1328.80it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b88e1d72d26c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-b88e1d72d26c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    524\u001b[0m     )\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     test_inputs, test_masks, test_labels = encode_dataset(\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0mtest_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_to_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     )\n",
            "\u001b[0;32m<ipython-input-6-b88e1d72d26c>\u001b[0m in \u001b[0;36mencode_dataset\u001b[0;34m(texts, tags, tokenizer, tag_to_id, max_length)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Tokenize the word and count resulting tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mword_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# Add the tokenized word to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    696\u001b[0m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m         \u001b[0;31m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             for token in self.basic_tokenizer.tokenize(\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msplit_special_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             ):\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;31m# union() returns a new set by concatenating the two sets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mnever_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnever_split\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xFFFD\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForTokenClassification,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from typing import Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def load_preprocessed_data(data_dir: str) -> Tuple[Dict, Dict]:\n",
        "    \"\"\"\n",
        "    Load preprocessed data for transformer models and tag mappings.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing the preprocessed data\n",
        "\n",
        "    Returns:\n",
        "        Tuple of transformer data and tag mappings\n",
        "    \"\"\"\n",
        "    transformer_dir = os.path.join(data_dir, \"processed\", \"transformer\")\n",
        "\n",
        "    # Load transformer data\n",
        "    with open(os.path.join(transformer_dir, \"transformer_data.pickle\"), \"rb\") as f:\n",
        "        transformer_data = pickle.load(f)\n",
        "\n",
        "    # Load tag mappings\n",
        "    with open(os.path.join(transformer_dir, \"tag_mappings.json\"), \"r\") as f:\n",
        "        tag_mappings = json.load(f)\n",
        "\n",
        "    return transformer_data, tag_mappings\n",
        "\n",
        "def encode_dataset(texts: List[List[str]], tags: List[List[str]],\n",
        "                  tokenizer, tag_to_id: Dict, max_length: int = 128) -> Tuple:\n",
        "    \"\"\"\n",
        "    Encode dataset using BERT tokenizer, handling subword tokenization.\n",
        "\n",
        "    Args:\n",
        "        texts: List of tokenized texts\n",
        "        tags: List of tags for each token\n",
        "        tokenizer: BERT tokenizer\n",
        "        tag_to_id: Mapping from tags to IDs\n",
        "        max_length: Maximum sequence length\n",
        "\n",
        "    Returns:\n",
        "        Tuple of encoded inputs, attention masks, and labels\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    labels = []\n",
        "\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    pad_token_label_id = tag_to_id.get(\"O\", -100)  # Use 'O' tag ID or -100 (ignored in loss)\n",
        "\n",
        "    for sentence_tokens, sentence_tags in tqdm(zip(texts, tags), total=len(texts), desc=\"Encoding dataset\"):\n",
        "        # Tokenize each word and align tags\n",
        "        bert_tokens = []\n",
        "        bert_labels = []\n",
        "\n",
        "        for word, tag in zip(sentence_tokens, sentence_tags):\n",
        "            # Tokenize the word and count resulting tokens\n",
        "            word_tokens = tokenizer.tokenize(word)\n",
        "\n",
        "            # Add the tokenized word to the output\n",
        "            bert_tokens.extend(word_tokens)\n",
        "\n",
        "            # Add the label for the first token\n",
        "            bert_labels.append(tag_to_id[tag])\n",
        "\n",
        "            # Add padding label for remaining subword tokens\n",
        "            bert_labels.extend([pad_token_label_id] * (len(word_tokens) - 1))\n",
        "\n",
        "        # Truncate sequences if they're longer than max_length\n",
        "        if len(bert_tokens) > max_length - 2:  # Account for [CLS] and [SEP]\n",
        "            bert_tokens = bert_tokens[:max_length - 2]\n",
        "            bert_labels = bert_labels[:max_length - 2]\n",
        "\n",
        "        # Add [CLS] and [SEP] tokens and corresponding labels\n",
        "        bert_tokens = [tokenizer.cls_token] + bert_tokens + [tokenizer.sep_token]\n",
        "        bert_labels = [pad_token_label_id] + bert_labels + [pad_token_label_id]\n",
        "\n",
        "        # Convert tokens to IDs\n",
        "        token_ids = tokenizer.convert_tokens_to_ids(bert_tokens)\n",
        "\n",
        "        # Calculate attention mask\n",
        "        attention_mask = [1] * len(token_ids)\n",
        "\n",
        "        # Pad sequences to max_length\n",
        "        padding_length = max_length - len(token_ids)\n",
        "\n",
        "        token_ids += [pad_token_id] * padding_length\n",
        "        attention_mask += [0] * padding_length\n",
        "        bert_labels += [pad_token_label_id] * padding_length\n",
        "\n",
        "        input_ids.append(token_ids)\n",
        "        attention_masks.append(attention_mask)\n",
        "        labels.append(bert_labels)\n",
        "\n",
        "    return torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(labels)\n",
        "\n",
        "def create_data_loaders(train_inputs, train_masks, train_labels,\n",
        "                       val_inputs=None, val_masks=None, val_labels=None,\n",
        "                       test_inputs=None, test_masks=None, test_labels=None,\n",
        "                       batch_size=32, num_workers=4) -> Dict:\n",
        "    \"\"\"\n",
        "    Create DataLoaders for training, validation, and testing.\n",
        "\n",
        "    Args:\n",
        "        *inputs, masks, labels: Tensor inputs for each dataset split\n",
        "        batch_size: Batch size for DataLoaders\n",
        "        num_workers: Number of worker processes for data loading\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of DataLoaders\n",
        "    \"\"\"\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    loaders = {\"train\": train_loader}\n",
        "\n",
        "    if val_inputs is not None:\n",
        "        val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "        val_loader = DataLoader(\n",
        "            val_data,\n",
        "            batch_size=batch_size,\n",
        "            pin_memory=True,\n",
        "            num_workers=num_workers\n",
        "        )\n",
        "        loaders[\"validation\"] = val_loader\n",
        "\n",
        "    if test_inputs is not None:\n",
        "        test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "        test_loader = DataLoader(\n",
        "            test_data,\n",
        "            batch_size=batch_size,\n",
        "            pin_memory=True,\n",
        "            num_workers=num_workers\n",
        "        )\n",
        "        loaders[\"test\"] = test_loader\n",
        "\n",
        "    return loaders\n",
        "\n",
        "def train_model(model, data_loaders, optimizer, scheduler, device,\n",
        "               num_epochs=3, evaluation_steps=100, id_to_tag=None,\n",
        "               gradient_accumulation_steps=1):\n",
        "    \"\"\"\n",
        "    Train the NER model and evaluate periodically.\n",
        "\n",
        "    Args:\n",
        "        model: BERT model for token classification\n",
        "        data_loaders: Dictionary of DataLoaders\n",
        "        optimizer: Optimizer for training\n",
        "        scheduler: Learning rate scheduler\n",
        "        device: Device to use for training\n",
        "        num_epochs: Number of training epochs\n",
        "        evaluation_steps: How often to evaluate on validation set\n",
        "        id_to_tag: Mapping from IDs to tags for metrics calculation\n",
        "        gradient_accumulation_steps: Number of steps to accumulate gradients\n",
        "\n",
        "    Returns:\n",
        "        Trained model and training history\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        # Enable cuDNN benchmarking for faster convolutions\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "        # Print GPU info\n",
        "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # Use mixed precision training with torch.cuda.amp\n",
        "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
        "\n",
        "    loss_fn = CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    # Initialize training history\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_f1\": [],\n",
        "        \"val_accuracy\": []\n",
        "    }\n",
        "\n",
        "    # Training loop\n",
        "    global_step = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        total_train_loss = 0\n",
        "        progress_bar = tqdm(data_loaders[\"train\"], desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for step, batch in enumerate(progress_bar):\n",
        "            # Extract batch and move to device\n",
        "            batch_inputs, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
        "\n",
        "            # Mixed precision training\n",
        "            if scaler:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # Forward pass\n",
        "                    outputs = model(\n",
        "                        input_ids=batch_inputs,\n",
        "                        attention_mask=batch_masks,\n",
        "                        labels=batch_labels\n",
        "                    )\n",
        "\n",
        "                    loss = outputs.loss / gradient_accumulation_steps\n",
        "\n",
        "                # Backward pass with gradient scaling\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Gradient accumulation\n",
        "                if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                    # Unscale gradients for clipping\n",
        "                    scaler.unscale_(optimizer)\n",
        "\n",
        "                    # Clip gradients to avoid explosion\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "                    # Update parameters with gradient scaling\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "\n",
        "                    # Update learning rate\n",
        "                    scheduler.step()\n",
        "\n",
        "                    # Clear gradients\n",
        "                    model.zero_grad()\n",
        "\n",
        "                    # Update global step\n",
        "                    global_step += 1\n",
        "            else:\n",
        "                # Standard training without mixed precision\n",
        "                # Forward pass\n",
        "                outputs = model(\n",
        "                    input_ids=batch_inputs,\n",
        "                    attention_mask=batch_masks,\n",
        "                    labels=batch_labels\n",
        "                )\n",
        "\n",
        "                loss = outputs.loss / gradient_accumulation_steps\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient accumulation\n",
        "                if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                    # Clip gradients to avoid explosion\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "                    # Update parameters\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    model.zero_grad()\n",
        "\n",
        "                    # Update global step\n",
        "                    global_step += 1\n",
        "\n",
        "            # Track loss (scaled back to original)\n",
        "            total_train_loss += loss.item() * gradient_accumulation_steps\n",
        "            progress_bar.set_postfix({\"loss\": loss.item() * gradient_accumulation_steps})\n",
        "\n",
        "            # Evaluate periodically\n",
        "            if global_step > 0 and global_step % evaluation_steps == 0 and \"validation\" in data_loaders:\n",
        "                # Evaluate on validation set\n",
        "                val_metrics = evaluate_model(model, data_loaders[\"validation\"], device, id_to_tag)\n",
        "\n",
        "                # Record metrics\n",
        "                history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
        "                history[\"val_f1\"].append(val_metrics[\"f1\"])\n",
        "                history[\"val_accuracy\"].append(val_metrics[\"accuracy\"])\n",
        "\n",
        "                # Print progress\n",
        "                print(f\"\\nStep {global_step}: Validation Loss: {val_metrics['loss']:.4f}, \"\n",
        "                      f\"F1: {val_metrics['f1']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "\n",
        "                # Back to training mode\n",
        "                model.train()\n",
        "\n",
        "        # Epoch-level statistics\n",
        "        avg_train_loss = total_train_loss / len(data_loaders[\"train\"])\n",
        "        history[\"train_loss\"].append(avg_train_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validate at the end of each epoch\n",
        "        if \"validation\" in data_loaders:\n",
        "            val_metrics = evaluate_model(model, data_loaders[\"validation\"], device, id_to_tag)\n",
        "            print(f\"Validation - Loss: {val_metrics['loss']:.4f}, \"\n",
        "                  f\"F1: {val_metrics['f1']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def evaluate_model(model, data_loader, device, id_to_tag=None):\n",
        "    \"\"\"\n",
        "    Evaluate model on a dataset.\n",
        "\n",
        "    Args:\n",
        "        model: BERT model for token classification\n",
        "        data_loader: DataLoader for evaluation\n",
        "        device: Device to use for evaluation\n",
        "        id_to_tag: Mapping from IDs to tags for metrics calculation\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of evaluation metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    loss_fn = CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            batch_inputs, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
        "\n",
        "            # Use mixed precision for evaluation too if available\n",
        "            if torch.cuda.is_available():\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(\n",
        "                        input_ids=batch_inputs,\n",
        "                        attention_mask=batch_masks\n",
        "                    )\n",
        "\n",
        "                    # Calculate loss\n",
        "                    active_loss = batch_masks.view(-1) == 1\n",
        "                    active_logits = outputs.logits.view(-1, model.config.num_labels)\n",
        "                    active_labels = torch.where(\n",
        "                        active_loss,\n",
        "                        batch_labels.view(-1),\n",
        "                        torch.tensor(-100).type_as(batch_labels)\n",
        "                    )\n",
        "\n",
        "                    loss = loss_fn(active_logits, active_labels)\n",
        "            else:\n",
        "                outputs = model(\n",
        "                    input_ids=batch_inputs,\n",
        "                    attention_mask=batch_masks\n",
        "                )\n",
        "\n",
        "                # Calculate loss\n",
        "                active_loss = batch_masks.view(-1) == 1\n",
        "                active_logits = outputs.logits.view(-1, model.config.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss,\n",
        "                    batch_labels.view(-1),\n",
        "                    torch.tensor(-100).type_as(batch_labels)\n",
        "                )\n",
        "\n",
        "                loss = loss_fn(active_logits, active_labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            logits = outputs.logits\n",
        "            batch_preds = torch.argmax(logits, dim=2)\n",
        "\n",
        "            # Convert to CPU and numpy for metric calculation\n",
        "            labels = batch_labels.detach().cpu().numpy()\n",
        "            preds = batch_preds.detach().cpu().numpy()\n",
        "            mask = batch_masks.detach().cpu().numpy()\n",
        "\n",
        "            # Collect only non-padding and non-special tokens (where label != -100)\n",
        "            for i in range(labels.shape[0]):\n",
        "                for j in range(labels.shape[1]):\n",
        "                    if labels[i, j] != -100 and mask[i, j] == 1:\n",
        "                        true_labels.append(labels[i, j])\n",
        "                        predicted_labels.append(preds[i, j])\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        \"loss\": total_loss / len(data_loader)\n",
        "    }\n",
        "\n",
        "    # Add more detailed metrics if id_to_tag is provided\n",
        "    if len(true_labels) > 0:\n",
        "        # Convert IDs back to string labels for better interpretability\n",
        "        if id_to_tag:\n",
        "            id_to_tag = {int(k): v for k, v in id_to_tag.items()}\n",
        "            true_tags = [id_to_tag.get(label, \"O\") for label in true_labels]\n",
        "            pred_tags = [id_to_tag.get(label, \"O\") for label in predicted_labels]\n",
        "\n",
        "            # Filter out \"O\" tag for entity-level metrics\n",
        "            entity_true = [label for label in true_tags if label != \"O\"]\n",
        "            entity_pred = [pred_tags[i] for i, label in enumerate(true_tags) if label != \"O\"]\n",
        "\n",
        "            # Entity-level metrics\n",
        "            entity_precision, entity_recall, entity_f1, _ = precision_recall_fscore_support(\n",
        "                entity_true, entity_pred, average='micro', zero_division=0\n",
        "            )\n",
        "\n",
        "            metrics[\"entity_precision\"] = entity_precision\n",
        "            metrics[\"entity_recall\"] = entity_recall\n",
        "            metrics[\"entity_f1\"] = entity_f1\n",
        "\n",
        "        # Token-level metrics (all tokens including \"O\")\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            true_labels, predicted_labels, average='micro', zero_division=0\n",
        "        )\n",
        "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "        metrics.update({\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"accuracy\": accuracy\n",
        "        })\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def visualize_training_history(history, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize training history.\n",
        "\n",
        "    Args:\n",
        "        history: Dictionary containing training history\n",
        "        save_path: Path to save the plot (optional)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history[\"train_loss\"], label=\"Training Loss\")\n",
        "    if \"val_loss\" in history and history[\"val_loss\"]:\n",
        "        plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    plt.title(\"Loss During Training\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot metrics\n",
        "    plt.subplot(1, 2, 2)\n",
        "    if \"val_f1\" in history and history[\"val_f1\"]:\n",
        "        plt.plot(history[\"val_f1\"], label=\"F1 Score\")\n",
        "    if \"val_accuracy\" in history and history[\"val_accuracy\"]:\n",
        "        plt.plot(history[\"val_accuracy\"], label=\"Accuracy\")\n",
        "    plt.title(\"Metrics During Training\")\n",
        "    plt.xlabel(\"Evaluation Step\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to train and evaluate a BERT-based NER model.\n",
        "    \"\"\"\n",
        "    # Set device (GPU if available)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Optimize GPU settings\n",
        "    if torch.cuda.is_available():\n",
        "        # Set optimal GPU memory allocation\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_max_memory_allocated()\n",
        "        torch.cuda.set_device(0)\n",
        "\n",
        "        # Print GPU info\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    # Set the number of worker processes based on CPU cores\n",
        "    num_workers = min(os.cpu_count(), 8) if os.cpu_count() else 4\n",
        "    print(f\"Using {num_workers} dataloader workers\")\n",
        "\n",
        "    # Load preprocessed data\n",
        "    data_dir = \"/content/data\"\n",
        "    transformer_data, tag_mappings = load_preprocessed_data(data_dir)\n",
        "\n",
        "    # Get tag mappings\n",
        "    tag_to_id = tag_mappings[\"tag_to_id\"]\n",
        "    id_to_tag = tag_mappings[\"id_to_tag\"]\n",
        "\n",
        "    # Extract the datasets - use smaller datasets for faster training\n",
        "    train_texts = transformer_data[\"train\"][\"texts\"][:5000]  # Use subset of training data\n",
        "    train_tags = transformer_data[\"train\"][\"tags\"][:5000]\n",
        "    dev_texts = transformer_data[\"dev\"][\"texts\"]\n",
        "    dev_tags = transformer_data[\"dev\"][\"tags\"]\n",
        "    test_texts = transformer_data[\"test\"][\"texts\"]\n",
        "    test_tags = transformer_data[\"test\"][\"tags\"]\n",
        "\n",
        "    print(f\"Training set: {len(train_texts)} examples\")\n",
        "    print(f\"Validation set: {len(dev_texts)} examples\")\n",
        "    print(f\"Test set: {len(test_texts)} examples\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "    # Encode datasets\n",
        "    print(\"Encoding datasets...\")\n",
        "    max_length = 128  # Adjust based on your dataset\n",
        "\n",
        "    train_inputs, train_masks, train_labels = encode_dataset(\n",
        "        train_texts, train_tags, tokenizer, tag_to_id, max_length\n",
        "    )\n",
        "\n",
        "    dev_inputs, dev_masks, dev_labels = encode_dataset(\n",
        "        dev_texts, dev_tags, tokenizer, tag_to_id, max_length\n",
        "    )\n",
        "\n",
        "    test_inputs, test_masks, test_labels = encode_dataset(\n",
        "        test_texts, test_tags, tokenizer, tag_to_id, max_length\n",
        "    )\n",
        "\n",
        "    print(\"Datasets encoded successfully!\")\n",
        "\n",
        "    # Set optimized hyperparameters (based on common best practices)\n",
        "    best_params = {\n",
        "        \"learning_rate\": 3e-5,\n",
        "        \"batch_size\": 32,  # Larger batch size for faster training\n",
        "        \"weight_decay\": 0.01,\n",
        "        \"gradient_accumulation_steps\": 2  # Accumulate gradients for larger effective batch\n",
        "    }\n",
        "\n",
        "    print(f\"\\nUsing hyperparameters: {best_params}\")\n",
        "\n",
        "    # Create data loaders for training\n",
        "    batch_size = best_params[\"batch_size\"]\n",
        "    data_loaders = create_data_loaders(\n",
        "        train_inputs, train_masks, train_labels,\n",
        "        dev_inputs, dev_masks, dev_labels,\n",
        "        test_inputs, test_masks, test_labels,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"\\nInitializing model...\")\n",
        "    model = BertForTokenClassification.from_pretrained(\n",
        "        \"bert-base-cased\",\n",
        "        num_labels=len(tag_to_id)\n",
        "    )\n",
        "\n",
        "    # Set up optimizer and scheduler\n",
        "    learning_rate = best_params[\"learning_rate\"]\n",
        "    weight_decay = best_params[\"weight_decay\"]\n",
        "    gradient_accumulation_steps = best_params[\"gradient_accumulation_steps\"]\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=learning_rate,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    # Use fewer epochs for faster training\n",
        "    num_epochs = 3\n",
        "    total_steps = len(data_loaders[\"train\"]) * num_epochs // gradient_accumulation_steps\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1 * total_steps),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"\\nTraining model...\")\n",
        "    model, history = train_model(\n",
        "        model, data_loaders, optimizer, scheduler,\n",
        "        device, num_epochs=num_epochs,\n",
        "        evaluation_steps=len(data_loaders[\"train\"]) // 2,  # Evaluate twice per epoch\n",
        "        id_to_tag=id_to_tag,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps\n",
        "    )\n",
        "\n",
        "    # Visualize training history\n",
        "    print(\"Visualizing training history...\")\n",
        "    os.makedirs(os.path.join(data_dir, \"visualizations\"), exist_ok=True)\n",
        "    visualize_training_history(\n",
        "        history,\n",
        "        save_path=os.path.join(data_dir, \"visualizations\", \"bert_training_history.png\")\n",
        "    )\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    test_metrics = evaluate_model(model, data_loaders[\"test\"], device, id_to_tag)\n",
        "\n",
        "    print(\"\\nTest Set Metrics:\")\n",
        "    print(f\"Loss: {test_metrics['loss']:.4f}\")\n",
        "    print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
        "    print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "\n",
        "    if \"entity_f1\" in test_metrics:\n",
        "        print(f\"\\nEntity-Level Metrics:\")\n",
        "        print(f\"Precision: {test_metrics['entity_precision']:.4f}\")\n",
        "        print(f\"Recall: {test_metrics['entity_recall']:.4f}\")\n",
        "        print(f\"F1 Score: {test_metrics['entity_f1']:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    print(\"\\nSaving model...\")\n",
        "    output_dir = os.path.join(data_dir, \"models\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    model_path = os.path.join(output_dir, \"bert_ner_model\")\n",
        "    model.save_pretrained(model_path)\n",
        "    tokenizer.save_pretrained(model_path)\n",
        "\n",
        "    # Save model configuration and mappings\n",
        "    with open(os.path.join(model_path, \"tag_mappings.json\"), \"w\") as f:\n",
        "        json.dump(tag_mappings, f, indent=2)\n",
        "\n",
        "    # Save test metrics\n",
        "    with open(os.path.join(model_path, \"test_metrics.json\"), \"w\") as f:\n",
        "        json.dump({k: float(v) for k, v in test_metrics.items()}, f, indent=2)\n",
        "\n",
        "    print(f\"\\nModel saved at {model_path}\")\n",
        "    print(\"\\nDone!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}