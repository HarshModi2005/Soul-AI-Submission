{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkxHMm7g6SEX"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "===============================================================================\n",
        "Download and Convert CoNLL-2003 Dataset to Raw CoNLL Format\n",
        "===============================================================================\n",
        "This script downloads the CoNLL-2003 dataset from Hugging Face, converts it\n",
        "into the raw CoNLL format, and saves the resulting files to disk. The steps\n",
        "include:\n",
        "\n",
        "  1. Directory Setup:\n",
        "     - Ensure required directories exist for storing raw and processed data.\n",
        "\n",
        "  2. Dataset Download:\n",
        "     - Load the CoNLL-2003 dataset using the Hugging Face 'datasets' library.\n",
        "\n",
        "  3. Data Conversion:\n",
        "     - Convert the dataset's token and numeric NER tag format into the standard\n",
        "       CoNLL format.\n",
        "     - Map numeric tags to their corresponding string representations.\n",
        "\n",
        "  4. File Saving:\n",
        "     - Save the converted data into separate files for train, validation, and test splits.\n",
        "\n",
        "  5. Next Steps:\n",
        "     - Instructions on subsequent commands for data preprocessing, training,\n",
        "       and starting an API.\n",
        "\n",
        "===============================================================================\n",
        "Author: Your Name\n",
        "Date: 2025-03-20\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# Standard Library Imports\n",
        "# =============================================================================\n",
        "import os  # For file and directory operations\n",
        "\n",
        "# =============================================================================\n",
        "# Third-Party Library Imports\n",
        "# =============================================================================\n",
        "from datasets import load_dataset  # For downloading datasets from Hugging Face\n",
        "import spacy  # Optional: can be used later for further processing (not used directly here)\n",
        "from spacy.tokens import DocBin  # Optional: for serializing Spacy Docs (not used here)\n",
        "\n",
        "# =============================================================================\n",
        "# Directory Setup\n",
        "# =============================================================================\n",
        "# Create directories to store raw CoNLL data and processed data if they don't exist.\n",
        "os.makedirs(\"data/conll2003\", exist_ok=True)\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "\n",
        "# =============================================================================\n",
        "# Dataset Download\n",
        "# =============================================================================\n",
        "print(\"Downloading CoNLL-2003 dataset from HuggingFace...\")\n",
        "\n",
        "# Load the CoNLL-2003 dataset using the 'datasets' library. This automatically\n",
        "# downloads and caches the dataset.\n",
        "dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "print(\"Download complete!\")\n",
        "\n",
        "# =============================================================================\n",
        "# Data Conversion: Save Raw Data in CoNLL Format\n",
        "# =============================================================================\n",
        "print(\"Converting to CoNLL format...\")\n",
        "\n",
        "# Iterate over each split of the dataset (train, validation, test)\n",
        "for split in [\"train\", \"validation\", \"test\"]:\n",
        "    # Determine the output filename based on the split:\n",
        "    # - \"eng.train\" for training split.\n",
        "    # - \"eng.testa\" for validation split.\n",
        "    # - \"eng.testb\" for test split.\n",
        "    output_file = (\n",
        "        f\"data/conll2003/eng.train\"\n",
        "        if split == \"train\" else\n",
        "        f\"data/conll2003/eng.testa\"\n",
        "        if split == \"validation\" else\n",
        "        f\"data/conll2003/eng.testb\"\n",
        "    )\n",
        "    print(f\"Processing {split} set -> {output_file}\")\n",
        "\n",
        "    # Open the output file for writing in UTF-8 encoding.\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        # Iterate over each example (sentence) in the current dataset split.\n",
        "        for example in dataset[split]:\n",
        "            # Each example contains a list of tokens and corresponding numeric NER tags.\n",
        "            for token, tag in zip(example[\"tokens\"], example[\"ner_tags\"]):\n",
        "                # Initialize default tag as \"O\" (outside any named entity).\n",
        "                tag_str = \"O\"\n",
        "                if tag > 0:\n",
        "                    # Map the numeric tag to its string representation.\n",
        "                    # The following mapping is specific to the CoNLL-2003 dataset.\n",
        "                    tag_map = {\n",
        "                        1: \"B-PER\", 2: \"I-PER\",\n",
        "                        3: \"B-ORG\", 4: \"I-ORG\",\n",
        "                        5: \"B-LOC\", 6: \"I-LOC\",\n",
        "                        7: \"B-MISC\", 8: \"I-MISC\"\n",
        "                    }\n",
        "                    tag_str = tag_map[tag]\n",
        "                # Write the token and its corresponding tag to the file.\n",
        "                f.write(f\"{token} {tag_str}\\n\")\n",
        "            # Write an empty line after each sentence to separate sentences.\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "print(\"Dataset downloaded and converted to CoNLL format successfully!\")\n",
        "print(\"Files created:\")\n",
        "print(\"  - data/conll2003/eng.train\")\n",
        "print(\"  - data/conll2003/eng.testa\")\n",
        "print(\"  - data/conll2003/eng.testb\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Run preprocessing: python -m src.preprocessing.data_loader\")\n",
        "print(\"2. Train model: python -m src.training.train_model\")\n",
        "print(\"3. Start API: uvicorn src.api.main:app --host 0.0.0.0 --port 8000\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlXTj26zBZTb"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "===============================================================================\n",
        "CoNLL-2003 Preprocessing Pipeline\n",
        "===============================================================================\n",
        "This script implements a comprehensive preprocessing pipeline for the CoNLL-2003\n",
        "dataset, typically used for Named Entity Recognition (NER) tasks. The pipeline\n",
        "covers the following steps:\n",
        "\n",
        "  1. Resource Setup:\n",
        "     - Download required NLTK resources.\n",
        "     - Verify and install the necessary SpaCy model.\n",
        "\n",
        "  2. Data Ingestion:\n",
        "     - Read and parse CoNLL-2003 formatted files into a structured format.\n",
        "\n",
        "  3. Data Exploration:\n",
        "     - Compute dataset statistics (e.g., sentence counts, word counts, entity\n",
        "       distributions, sentence lengths).\n",
        "     - Visualize entity distributions using bar charts.\n",
        "\n",
        "  4. Text Preprocessing:\n",
        "     - Clean and normalize text using both NLTK and SpaCy libraries.\n",
        "     - Optionally remove stopwords and perform lemmatization.\n",
        "\n",
        "  5. Data Conversion:\n",
        "     - Convert parsed data into different formats: SpaCy training format, BIO\n",
        "       format, and JSON format.\n",
        "     - Prepare data for transformer-based models (e.g., tokenized texts and\n",
        "       corresponding labels).\n",
        "\n",
        "  6. Data Saving:\n",
        "     - Persist processed data and label mappings to disk for downstream tasks.\n",
        "\n",
        "===============================================================================\n",
        "Author: Your Name\n",
        "Date: 2025-03-20\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# Standard Library Imports\n",
        "# =============================================================================\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import subprocess\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "# =============================================================================\n",
        "# Third-Party Imports\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# =============================================================================\n",
        "# NLTK Resource Downloads\n",
        "# =============================================================================\n",
        "# Download required NLTK resources for text tokenization, stopword removal,\n",
        "# and lemmatization. The 'quiet=True' flag prevents excessive console output.\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)  # Confirm if 'punkt_tab' is needed for your tasks.\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# =============================================================================\n",
        "# SpaCy Model Installation and Loading\n",
        "# =============================================================================\n",
        "def ensure_spacy_model(model_name: str = \"en_core_web_sm\") -> None:\n",
        "    \"\"\"\n",
        "    Verify that the specified SpaCy model is installed. If not, install the model.\n",
        "\n",
        "    The function attempts to load the model and prints a confirmation message if\n",
        "    successful. If the model is missing, it uses the SpaCy CLI via subprocess to\n",
        "    download and install the model.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): The name of the SpaCy model to load. Defaults to \"en_core_web_sm\".\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Attempt to load the specified SpaCy model.\n",
        "        spacy.load(model_name)\n",
        "        print(f\"SpaCy model '{model_name}' is already installed.\")\n",
        "    except OSError:\n",
        "        # Handle the case where the model is not installed.\n",
        "        print(f\"SpaCy model '{model_name}' not found. Installing...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", model_name])\n",
        "        print(f\"SpaCy model '{model_name}' has been installed.\")\n",
        "\n",
        "# Ensure the SpaCy model is available before continuing.\n",
        "ensure_spacy_model()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# =============================================================================\n",
        "# Data Reading and Parsing Functions\n",
        "# =============================================================================\n",
        "def read_conll_file(file_path: str) -> List[List[Tuple[str, str]]]:\n",
        "    \"\"\"\n",
        "    Parse a CoNLL-2003 formatted file into a structured list of sentences.\n",
        "\n",
        "    Each sentence is represented as a list of tuples, where each tuple contains\n",
        "    a token (word) and its corresponding annotation tag (e.g., 'B-PER', 'I-LOC', 'O').\n",
        "\n",
        "    The function handles:\n",
        "      - Skipping of metadata and comment lines (e.g., \"-DOCSTART-\").\n",
        "      - Correct grouping of tokens into sentences based on blank lines.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Full file path to the CoNLL file.\n",
        "\n",
        "    Returns:\n",
        "        List[List[Tuple[str, str]]]: List of sentences with each sentence represented\n",
        "                                     as a list of (word, tag) tuples.\n",
        "    \"\"\"\n",
        "    sentences = []         # Holds all parsed sentences.\n",
        "    current_sentence = []  # Temporarily accumulates tokens for the current sentence.\n",
        "\n",
        "    # Open the file with UTF-8 encoding to support diverse characters.\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()  # Remove extra whitespace.\n",
        "\n",
        "            # Skip empty lines and metadata lines (these indicate sentence boundaries).\n",
        "            if not line or line.startswith('-DOCSTART-') or line.startswith('//'):\n",
        "                if current_sentence:\n",
        "                    # Append the accumulated sentence before resetting.\n",
        "                    sentences.append(current_sentence)\n",
        "                    current_sentence = []\n",
        "                continue\n",
        "\n",
        "            # Split the line into parts; the first part is the token, last is the tag.\n",
        "            parts = line.split()\n",
        "            if len(parts) >= 2:\n",
        "                word = parts[0]\n",
        "                tag = parts[-1]  # Assumes the tag is the final element.\n",
        "                current_sentence.append((word, tag))\n",
        "\n",
        "    # Ensure the last sentence is added if the file does not end with a blank line.\n",
        "    if current_sentence:\n",
        "        sentences.append(current_sentence)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# =============================================================================\n",
        "# Dataset Exploration and Visualization Functions\n",
        "# =============================================================================\n",
        "def explore_dataset(sentences: List[List[Tuple[str, str]]]) -> Dict:\n",
        "    \"\"\"\n",
        "    Compute and return a set of statistics describing the dataset.\n",
        "\n",
        "    The returned statistics include:\n",
        "      - Number of sentences.\n",
        "      - Total word count.\n",
        "      - Count of unique words (case-insensitive).\n",
        "      - Entity counts by type (e.g., PER, LOC).\n",
        "      - Average length of entities.\n",
        "      - Average and maximum sentence lengths.\n",
        "\n",
        "    Args:\n",
        "        sentences (List[List[Tuple[str, str]]]): Structured sentences with (word, tag) tuples.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Dictionary containing various computed statistics.\n",
        "    \"\"\"\n",
        "    num_sentences = len(sentences)\n",
        "    total_words = sum(len(sentence) for sentence in sentences)\n",
        "\n",
        "    # Use a set to count unique words; convert to lowercase for consistency.\n",
        "    unique_words = set()\n",
        "    for sentence in sentences:\n",
        "        for word, _ in sentence:\n",
        "            unique_words.add(word.lower())\n",
        "    num_unique_words = len(unique_words)\n",
        "\n",
        "    # Initialize containers to track entity counts and their lengths.\n",
        "    entity_counts = Counter()\n",
        "    entity_length_distribution = {}\n",
        "    current_entity = None\n",
        "    current_entity_length = 0\n",
        "\n",
        "    # Loop through every sentence and token to compute entity-based statistics.\n",
        "    for sentence in sentences:\n",
        "        for _, tag in sentence:\n",
        "            if tag.startswith('B-'):\n",
        "                # If a new entity begins, finish tracking the previous entity.\n",
        "                if current_entity:\n",
        "                    entity_length_distribution.setdefault(current_entity, []).append(current_entity_length)\n",
        "                current_entity = tag[2:]  # Remove the \"B-\" prefix to get the entity type.\n",
        "                current_entity_length = 1\n",
        "                entity_counts[current_entity] += 1\n",
        "            elif tag.startswith('I-'):\n",
        "                # If tag indicates a continuation and matches the current entity type, increment length.\n",
        "                if current_entity == tag[2:]:\n",
        "                    current_entity_length += 1\n",
        "            else:\n",
        "                # Encountered a non-entity token; finalize the current entity if one is being tracked.\n",
        "                if current_entity:\n",
        "                    entity_length_distribution.setdefault(current_entity, []).append(current_entity_length)\n",
        "                    current_entity = None\n",
        "                    current_entity_length = 0\n",
        "\n",
        "    # Calculate the average length for each entity type.\n",
        "    avg_entity_length = {\n",
        "        entity: (sum(lengths) / len(lengths) if lengths else 0)\n",
        "        for entity, lengths in entity_length_distribution.items()\n",
        "    }\n",
        "\n",
        "    # Compute sentence length metrics.\n",
        "    sentence_lengths = [len(sentence) for sentence in sentences]\n",
        "    avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0\n",
        "    max_sentence_length = max(sentence_lengths) if sentence_lengths else 0\n",
        "\n",
        "    # Return a dictionary of computed statistics.\n",
        "    return {\n",
        "        'num_sentences': num_sentences,\n",
        "        'total_words': total_words,\n",
        "        'num_unique_words': num_unique_words,\n",
        "        'entity_counts': dict(entity_counts),\n",
        "        'avg_entity_length': avg_entity_length,\n",
        "        'avg_sentence_length': avg_sentence_length,\n",
        "        'max_sentence_length': max_sentence_length\n",
        "    }\n",
        "\n",
        "\n",
        "def visualize_entity_distribution(stats: Dict, save_path: Optional[str] = None) -> None:\n",
        "    \"\"\"\n",
        "    Create and display or save a bar chart of entity type distribution.\n",
        "\n",
        "    The function extracts entity counts from the statistics dictionary and uses\n",
        "    Matplotlib to create a bar chart. If a save path is provided, the chart is\n",
        "    saved to a file; otherwise, it is displayed interactively.\n",
        "\n",
        "    Args:\n",
        "        stats (Dict): Dataset statistics (must contain 'entity_counts').\n",
        "        save_path (Optional[str]): File path for saving the plot. If None, plot is shown.\n",
        "    \"\"\"\n",
        "    # Set up a figure with a defined size.\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Retrieve entity names and counts from the provided statistics.\n",
        "    entity_counts = stats['entity_counts']\n",
        "    entities = list(entity_counts.keys())\n",
        "    counts = list(entity_counts.values())\n",
        "\n",
        "    # Create the bar chart.\n",
        "    plt.bar(entities, counts)\n",
        "    plt.title('Distribution of Entity Types')\n",
        "    plt.xlabel('Entity Type')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Save the plot to a file if a path is provided; otherwise, display it.\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# =============================================================================\n",
        "# Text Preprocessing Functions (Using NLTK and SpaCy)\n",
        "# =============================================================================\n",
        "def preprocess_text(text: str, remove_stopwords: bool = True, lemmatize: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Preprocess a text string using NLTK libraries.\n",
        "\n",
        "    The preprocessing steps include:\n",
        "      - Converting text to lowercase.\n",
        "      - Tokenizing the text into words.\n",
        "      - Optionally removing stopwords.\n",
        "      - Optionally performing lemmatization to reduce words to their base forms.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to be processed.\n",
        "        remove_stopwords (bool): Flag indicating whether to remove stopwords. Defaults to True.\n",
        "        lemmatize (bool): Flag indicating whether to perform lemmatization. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed text as a single space-separated string.\n",
        "    \"\"\"\n",
        "    # Normalize text by converting to lowercase.\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize text using NLTK's word_tokenize.\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords if enabled.\n",
        "    if remove_stopwords:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Perform lemmatization if enabled.\n",
        "    if lemmatize:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Return the preprocessed text.\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "def preprocess_spacy(text: str, remove_stopwords: bool = True, lemmatize: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Preprocess a text string using SpaCy for advanced tokenization and linguistic processing.\n",
        "\n",
        "    The function leverages SpaCy to:\n",
        "      - Tokenize the input text.\n",
        "      - Optionally remove stopwords based on SpaCy's built-in list.\n",
        "      - Optionally perform lemmatization.\n",
        "      - Convert tokens to lowercase.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to be processed.\n",
        "        remove_stopwords (bool): Flag indicating whether to remove stopwords. Defaults to True.\n",
        "        lemmatize (bool): Flag indicating whether to use token lemmas. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        str: Processed text as a single string.\n",
        "    \"\"\"\n",
        "    # Process text to create a SpaCy Doc object.\n",
        "    doc = nlp(text)\n",
        "    tokens = []\n",
        "\n",
        "    # Iterate through each token in the document.\n",
        "    for token in doc:\n",
        "        # Skip tokens that are identified as stopwords if removal is enabled.\n",
        "        if remove_stopwords and token.is_stop:\n",
        "            continue\n",
        "\n",
        "        # Choose the token's lemma or its original text based on the flag.\n",
        "        processed_token = token.lemma_ if lemmatize else token.text\n",
        "        processed_token = processed_token.lower()  # Ensure consistency.\n",
        "        tokens.append(processed_token)\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# =============================================================================\n",
        "# Data Conversion Functions for Downstream Tasks\n",
        "# =============================================================================\n",
        "def convert_to_spacy_format(sentences: List[List[Tuple[str, str]]], output_file: str) -> List:\n",
        "    \"\"\"\n",
        "    Convert parsed CoNLL data into a format suitable for SpaCy training.\n",
        "\n",
        "    The output format is a list of tuples, where each tuple contains:\n",
        "      - A full sentence as a string.\n",
        "      - A dictionary with a key 'entities' mapping to a list of tuples.\n",
        "        Each tuple in 'entities' contains (character_start, character_end, entity_type).\n",
        "\n",
        "    The formatted data is saved as a pickle file.\n",
        "\n",
        "    Args:\n",
        "        sentences (List[List[Tuple[str, str]]]): Parsed sentences with (word, tag) tuples.\n",
        "        output_file (str): File path where the output pickle file is saved.\n",
        "\n",
        "    Returns:\n",
        "        List: A list of training data formatted for SpaCy.\n",
        "    \"\"\"\n",
        "    training_data = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Extract words and corresponding tags from the sentence.\n",
        "        words = [word for word, _ in sentence]\n",
        "        tags = [tag for _, tag in sentence]\n",
        "        text = ' '.join(words)\n",
        "        entities = []\n",
        "        i = 0  # Initialize token index.\n",
        "\n",
        "        # Iterate over tokens to determine entity boundaries using the BIO scheme.\n",
        "        while i < len(tags):\n",
        "            if tags[i].startswith('B-'):\n",
        "                entity_type = tags[i][2:]  # Extract entity type by removing the 'B-' prefix.\n",
        "                start = i\n",
        "                end = i + 1\n",
        "\n",
        "                # Extend the entity span through consecutive 'I-' tags.\n",
        "                while end < len(tags) and tags[end].startswith('I-') and tags[end][2:] == entity_type:\n",
        "                    end += 1\n",
        "\n",
        "                # Calculate character offsets for the entity in the sentence.\n",
        "                char_start = len(' '.join(words[:start]))\n",
        "                if start > 0:\n",
        "                    char_start += 1  # Adjust for the whitespace.\n",
        "                char_end = char_start + len(' '.join(words[start:end]))\n",
        "\n",
        "                # Append the entity span information.\n",
        "                entities.append((char_start, char_end, entity_type))\n",
        "                i = end  # Move index to the end of the current entity.\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "        training_data.append((text, {'entities': entities}))\n",
        "\n",
        "    # Save the formatted data in pickle format for efficient reloading.\n",
        "    with open(output_file, 'wb') as f:\n",
        "        pickle.dump(training_data, f)\n",
        "\n",
        "    return training_data\n",
        "\n",
        "\n",
        "def convert_to_bio_format(sentences: List[List[Tuple[str, str]]], output_file: str) -> None:\n",
        "    \"\"\"\n",
        "    Convert dataset into BIO format and write to a text file.\n",
        "\n",
        "    Each token and its corresponding tag is written on a separate line, and\n",
        "    sentences are separated by an empty line. This format is commonly used for\n",
        "    NER model training and evaluation.\n",
        "\n",
        "    Args:\n",
        "        sentences (List[List[Tuple[str, str]]]): Parsed sentences with (word, tag) tuples.\n",
        "        output_file (str): File path where the BIO formatted text file will be saved.\n",
        "    \"\"\"\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for sentence in sentences:\n",
        "            for word, tag in sentence:\n",
        "                f.write(f\"{word} {tag}\\n\")\n",
        "            f.write(\"\\n\")  # Insert an empty line between sentences.\n",
        "\n",
        "\n",
        "def convert_to_json_format(sentences: List[List[Tuple[str, str]]], output_file: str) -> None:\n",
        "    \"\"\"\n",
        "    Convert dataset into JSON format and save it to a file.\n",
        "\n",
        "    Each sentence is represented as a JSON object with the following keys:\n",
        "      - 'text': The full sentence as a string.\n",
        "      - 'tokens': A list of individual tokens.\n",
        "      - 'tags': The corresponding list of tags for the tokens.\n",
        "\n",
        "    Args:\n",
        "        sentences (List[List[Tuple[str, str]]]): Parsed sentences with (word, tag) tuples.\n",
        "        output_file (str): File path where the JSON output will be saved.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for sentence in sentences:\n",
        "        words = [word for word, _ in sentence]\n",
        "        tags = [tag for _, tag in sentence]\n",
        "        data.append({\n",
        "            'text': ' '.join(words),\n",
        "            'tokens': words,\n",
        "            'tags': tags\n",
        "        })\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "\n",
        "def preprocess_data_for_transformers(sentences: List[List[Tuple[str, str]]]) -> Tuple[List, List]:\n",
        "    \"\"\"\n",
        "    Prepare data for transformer-based models by tokenizing the sentences and\n",
        "    collecting corresponding tag sequences.\n",
        "\n",
        "    The output consists of:\n",
        "      - A list where each element is a list of tokens from a sentence.\n",
        "      - A parallel list where each element is a list of tags for the sentence tokens.\n",
        "\n",
        "    Args:\n",
        "        sentences (List[List[Tuple[str, str]]]): Parsed sentences with (word, tag) tuples.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List, List]: Tokenized texts and their corresponding tag sequences.\n",
        "    \"\"\"\n",
        "    tokenized_texts = []\n",
        "    tags_list = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = [word for word, _ in sentence]\n",
        "        tags = [tag for _, tag in sentence]\n",
        "        tokenized_texts.append(words)\n",
        "        tags_list.append(tags)\n",
        "\n",
        "    return tokenized_texts, tags_list\n",
        "\n",
        "\n",
        "def create_entity_labels_mapping(sentences: List[List[Tuple[str, str]]]) -> Tuple[Dict, Dict]:\n",
        "    \"\"\"\n",
        "    Generate mapping dictionaries for entity labels to unique integer IDs and vice versa.\n",
        "\n",
        "    This is essential for model training, where labels are typically represented by integers.\n",
        "\n",
        "    Args:\n",
        "        sentences (List[List[Tuple[str, str]]]): Parsed sentences with (word, tag) tuples.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict, Dict]: A tuple containing:\n",
        "            - tag_to_id: Mapping from entity tag to a unique integer ID.\n",
        "            - id_to_tag: Reverse mapping from the integer ID to the entity tag.\n",
        "    \"\"\"\n",
        "    unique_tags = set()\n",
        "    for sentence in sentences:\n",
        "        for _, tag in sentence:\n",
        "            unique_tags.add(tag)\n",
        "\n",
        "    # Sort tags to ensure a consistent mapping order.\n",
        "    tag_to_id = {tag: i for i, tag in enumerate(sorted(list(unique_tags)))}\n",
        "    id_to_tag = {i: tag for tag, i in tag_to_id.items()}\n",
        "    return tag_to_id, id_to_tag\n",
        "\n",
        "# =============================================================================\n",
        "# Main Preprocessing Pipeline Execution\n",
        "# =============================================================================\n",
        "def main() -> None:\n",
        "    \"\"\"\n",
        "    Execute the complete preprocessing pipeline for the CoNLL-2003 dataset.\n",
        "\n",
        "    This includes:\n",
        "      - Reading and parsing the dataset.\n",
        "      - Computing and displaying dataset statistics.\n",
        "      - Visualizing entity distributions.\n",
        "      - Converting data to multiple formats (SpaCy, BIO, JSON).\n",
        "      - Preparing and saving data for transformer-based models.\n",
        "    \"\"\"\n",
        "    # Define directories and file paths for the dataset.\n",
        "    data_dir = \"/content/data\"\n",
        "    train_path = os.path.join(data_dir, \"conll2003\", \"eng.train\")\n",
        "    testa_path = os.path.join(data_dir, \"conll2003\", \"eng.testa\")\n",
        "    testb_path = os.path.join(data_dir, \"conll2003\", \"eng.testb\")\n",
        "\n",
        "    # Read datasets in CoNLL format.\n",
        "    train_sentences = read_conll_file(train_path)\n",
        "    dev_sentences = read_conll_file(testa_path)\n",
        "    test_sentences = read_conll_file(testb_path)\n",
        "\n",
        "    # Compute statistics for the training dataset.\n",
        "    train_stats = explore_dataset(train_sentences)\n",
        "    print(\"Training Dataset Statistics:\")\n",
        "    print(f\"  Number of sentences: {train_stats['num_sentences']}\")\n",
        "    print(f\"  Total words: {train_stats['total_words']}\")\n",
        "    print(f\"  Unique words: {train_stats['num_unique_words']}\")\n",
        "    print(f\"  Entity counts: {train_stats['entity_counts']}\")\n",
        "    print(f\"  Average sentence length: {train_stats['avg_sentence_length']}\")\n",
        "\n",
        "    # Visualize the entity distribution and save the plot to disk.\n",
        "    vis_dir = os.path.join(data_dir, \"visualizations\")\n",
        "    os.makedirs(vis_dir, exist_ok=True)\n",
        "    visualize_entity_distribution(\n",
        "        train_stats,\n",
        "        save_path=os.path.join(vis_dir, \"entity_distribution_train.png\")\n",
        "    )\n",
        "\n",
        "    # Create directories for saving processed data in various formats.\n",
        "    processed_dir = os.path.join(data_dir, \"processed\")\n",
        "    spacy_dir = os.path.join(processed_dir, \"spacy\")\n",
        "    bio_dir = os.path.join(processed_dir, \"bio\")\n",
        "    json_dir = os.path.join(processed_dir, \"json\")\n",
        "    transformer_dir = os.path.join(processed_dir, \"transformer\")\n",
        "    for directory in [spacy_dir, bio_dir, json_dir, transformer_dir]:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Convert and save the dataset in SpaCy training format.\n",
        "    print(\"Converting data to SpaCy format...\")\n",
        "    convert_to_spacy_format(train_sentences, os.path.join(spacy_dir, \"train.pickle\"))\n",
        "    convert_to_spacy_format(dev_sentences, os.path.join(spacy_dir, \"dev.pickle\"))\n",
        "    convert_to_spacy_format(test_sentences, os.path.join(spacy_dir, \"test.pickle\"))\n",
        "\n",
        "    # Convert and save the dataset in BIO format.\n",
        "    print(\"Converting data to BIO format...\")\n",
        "    convert_to_bio_format(train_sentences, os.path.join(bio_dir, \"train.txt\"))\n",
        "    convert_to_bio_format(dev_sentences, os.path.join(bio_dir, \"dev.txt\"))\n",
        "    convert_to_bio_format(test_sentences, os.path.join(bio_dir, \"test.txt\"))\n",
        "\n",
        "    # Convert and save the dataset in JSON format.\n",
        "    print(\"Converting data to JSON format...\")\n",
        "    convert_to_json_format(train_sentences, os.path.join(json_dir, \"train.json\"))\n",
        "    convert_to_json_format(dev_sentences, os.path.join(json_dir, \"dev.json\"))\n",
        "    convert_to_json_format(test_sentences, os.path.join(json_dir, \"test.json\"))\n",
        "\n",
        "    # Preprocess the data for transformer-based models.\n",
        "    print(\"Preparing data for transformer models...\")\n",
        "    train_tokens, train_tags = preprocess_data_for_transformers(train_sentences)\n",
        "    dev_tokens, dev_tags = preprocess_data_for_transformers(dev_sentences)\n",
        "    test_tokens, test_tags = preprocess_data_for_transformers(test_sentences)\n",
        "\n",
        "    # Generate and save label mappings for entity types.\n",
        "    tag_to_id, id_to_tag = create_entity_labels_mapping(train_sentences + dev_sentences + test_sentences)\n",
        "    mapping_file = os.path.join(transformer_dir, \"tag_mappings.json\")\n",
        "    with open(mapping_file, 'w', encoding='utf-8') as f:\n",
        "        # Convert integer keys to strings to ensure JSON compatibility.\n",
        "        json.dump({\n",
        "            \"tag_to_id\": tag_to_id,\n",
        "            \"id_to_tag\": {str(k): v for k, v in id_to_tag.items()}\n",
        "        }, f, indent=2)\n",
        "\n",
        "    # Package and save transformer-ready data as a pickle file.\n",
        "    transformer_data = {\n",
        "        \"train\": {\"texts\": train_tokens, \"tags\": train_tags},\n",
        "        \"dev\": {\"texts\": dev_tokens, \"tags\": dev_tags},\n",
        "        \"test\": {\"texts\": test_tokens, \"tags\": test_tags}\n",
        "    }\n",
        "    transformer_file = os.path.join(transformer_dir, \"transformer_data.pickle\")\n",
        "    with open(transformer_file, 'wb') as f:\n",
        "        pickle.dump(transformer_data, f)\n",
        "\n",
        "    # Demonstrate preprocessing capabilities using a sample from the training dataset.\n",
        "    print(\"\\nText Preprocessing Examples:\")\n",
        "    if train_sentences:\n",
        "        # Create a sample text by joining the tokens of the first sentence.\n",
        "        sample_text = ' '.join([word for word, _ in train_sentences[0]])\n",
        "        print(f\"Original: {sample_text}\")\n",
        "        print(f\"NLTK processed: {preprocess_text(sample_text)}\")\n",
        "        print(f\"SpaCy processed: {preprocess_spacy(sample_text)}\")\n",
        "\n",
        "    print(\"\\nPreprocessing completed successfully!\")\n",
        "    print(f\"Processed data has been saved to the directory: {processed_dir}\")\n",
        "\n",
        "# =============================================================================\n",
        "# Script Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AopCL5XXBehR"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "===============================================================================\n",
        "BERT-based NER Training Pipeline with Hyperparameter Tuning\n",
        "===============================================================================\n",
        "This script implements a complete pipeline for training and evaluating a BERT-\n",
        "based Named Entity Recognition (NER) model. It performs the following steps:\n",
        "\n",
        "  1. Data Loading:\n",
        "     - Loads preprocessed transformer data and tag mappings from disk.\n",
        "\n",
        "  2. Data Encoding:\n",
        "     - Encodes tokenized texts and their labels using a BERT tokenizer.\n",
        "     - Handles subword tokenization and label alignment.\n",
        "\n",
        "  3. DataLoader Creation:\n",
        "     - Constructs PyTorch DataLoaders for training, validation, and testing.\n",
        "\n",
        "  4. Model Training:\n",
        "     - Trains the model using gradient accumulation and optionally mixed precision.\n",
        "     - Periodically evaluates the model on a validation set and logs metrics.\n",
        "\n",
        "  5. Model Evaluation:\n",
        "     - Evaluates the trained model by calculating token-level and entity-level metrics.\n",
        "\n",
        "  6. Hyperparameter Tuning:\n",
        "     - Conducts grid search over key hyperparameters (learning rate, batch size, etc.)\n",
        "       to determine the optimal configuration.\n",
        "\n",
        "  7. Visualization and Saving:\n",
        "     - Visualizes training history.\n",
        "     - Saves the trained model, tokenizer, tag mappings, and evaluation metrics.\n",
        "\n",
        "===============================================================================\n",
        "Author: Your Name\n",
        "Date: 2025-03-20\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# Standard Library Imports\n",
        "# =============================================================================\n",
        "import os          # File and directory operations\n",
        "import json        # Reading and writing JSON files\n",
        "import pickle      # Serializing and deserializing Python objects\n",
        "import numpy as np # Numerical operations and array handling\n",
        "from typing import Dict, List, Tuple  # Type annotations for clarity\n",
        "\n",
        "# =============================================================================\n",
        "# PyTorch and Related Imports\n",
        "# =============================================================================\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset  # Data loading utilities\n",
        "from torch.optim import AdamW                           # Optimizer with weight decay\n",
        "from torch.nn import CrossEntropyLoss                   # Loss function for classification\n",
        "\n",
        "# =============================================================================\n",
        "# Scikit-learn and Transformers Imports\n",
        "# =============================================================================\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score  # Metrics for evaluation\n",
        "from sklearn.model_selection import ParameterGrid  # Hyperparameter grid search\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForTokenClassification,\n",
        "    get_linear_schedule_with_warmup\n",
        ")  # Pretrained BERT modules and scheduling\n",
        "\n",
        "# =============================================================================\n",
        "# Visualization and Progress Bar Imports\n",
        "# =============================================================================\n",
        "import matplotlib.pyplot as plt  # Plotting library for visualization\n",
        "import seaborn as sns            # Advanced visualization (if needed)\n",
        "from tqdm import tqdm            # Progress bar for loops\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Data Loading Function\n",
        "# =============================================================================\n",
        "def load_preprocessed_data(data_dir: str) -> Tuple[Dict, Dict]:\n",
        "    \"\"\"\n",
        "    Load preprocessed transformer data and tag mappings from disk.\n",
        "\n",
        "    This function reads a pickle file for transformer data and a JSON file for tag mappings.\n",
        "    These files are expected to reside in the 'processed/transformer' subdirectory of the\n",
        "    provided data directory.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Root directory containing the processed data.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict, Dict]: A tuple where:\n",
        "            - The first element is the transformer-formatted data.\n",
        "            - The second element is the tag mappings (both tag-to-ID and ID-to-tag).\n",
        "    \"\"\"\n",
        "    transformer_dir = os.path.join(data_dir, \"processed\", \"transformer\")\n",
        "\n",
        "    # Load the transformer data from a pickle file.\n",
        "    with open(os.path.join(transformer_dir, \"transformer_data.pickle\"), \"rb\") as f:\n",
        "        transformer_data = pickle.load(f)\n",
        "\n",
        "    # Load the tag mappings from a JSON file.\n",
        "    with open(os.path.join(transformer_dir, \"tag_mappings.json\"), \"r\") as f:\n",
        "        tag_mappings = json.load(f)\n",
        "\n",
        "    return transformer_data, tag_mappings\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Data Encoding Function\n",
        "# =============================================================================\n",
        "def encode_dataset(texts: List[List[str]], tags: List[List[str]], tokenizer,\n",
        "                   tag_to_id: Dict, max_length: int = 128) -> Tuple:\n",
        "    \"\"\"\n",
        "    Encode the dataset using the BERT tokenizer while aligning token labels with subwords.\n",
        "\n",
        "    This function iterates over each sentence and its corresponding tag sequence.\n",
        "    For each word:\n",
        "      - It tokenizes the word into subword tokens.\n",
        "      - The first subword receives the actual label.\n",
        "      - Any subsequent subwords are assigned a pad label (either the \"O\" tag or -100, which is ignored in loss computation).\n",
        "\n",
        "    After processing, the function adds special tokens ([CLS] and [SEP]), truncates to the specified\n",
        "    max_length if necessary, and pads the sequence to ensure fixed length.\n",
        "\n",
        "    Args:\n",
        "        texts (List[List[str]]): List of tokenized sentences (each sentence is a list of words).\n",
        "        tags (List[List[str]]): List of tag sequences (each is a list of tags for the sentence).\n",
        "        tokenizer: Pretrained BERT tokenizer.\n",
        "        tag_to_id (Dict): Dictionary mapping tag strings to integer IDs.\n",
        "        max_length (int): Maximum allowed sequence length (default is 128).\n",
        "\n",
        "    Returns:\n",
        "        Tuple: Tensors for input IDs, attention masks, and labels.\n",
        "    \"\"\"\n",
        "    input_ids = []        # List to hold encoded token IDs for each sentence.\n",
        "    attention_masks = []  # List to hold attention masks (1 for tokens, 0 for padding).\n",
        "    labels = []           # List to hold aligned label IDs for each token.\n",
        "\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    # If available, use the ID for the \"O\" tag as padding; otherwise, use -100.\n",
        "    pad_token_label_id = tag_to_id.get(\"O\", -100)\n",
        "\n",
        "    # Process each sentence and its corresponding tags.\n",
        "    for sentence_tokens, sentence_tags in tqdm(zip(texts, tags), total=len(texts), desc=\"Encoding dataset\"):\n",
        "        bert_tokens = []  # This list will store the BERT subword tokens.\n",
        "        bert_labels = []  # This list will store the labels aligned with the subwords.\n",
        "\n",
        "        # Iterate through each word in the sentence.\n",
        "        for word, tag in zip(sentence_tokens, sentence_tags):\n",
        "            # Tokenize the word into subword tokens using the BERT tokenizer.\n",
        "            word_tokens = tokenizer.tokenize(word)\n",
        "            # Extend the overall token list with the subword tokens.\n",
        "            bert_tokens.extend(word_tokens)\n",
        "            # For the first subword, assign the true label.\n",
        "            bert_labels.append(tag_to_id[tag])\n",
        "            # For subsequent subwords, assign the pad label (so they don't affect loss).\n",
        "            bert_labels.extend([pad_token_label_id] * (len(word_tokens) - 1))\n",
        "\n",
        "        # Check if the tokenized sentence exceeds the maximum allowed length (minus special tokens).\n",
        "        if len(bert_tokens) > max_length - 2:\n",
        "            bert_tokens = bert_tokens[:max_length - 2]\n",
        "            bert_labels = bert_labels[:max_length - 2]\n",
        "\n",
        "        # Add special tokens: [CLS] token at the beginning and [SEP] token at the end.\n",
        "        bert_tokens = [tokenizer.cls_token] + bert_tokens + [tokenizer.sep_token]\n",
        "        bert_labels = [pad_token_label_id] + bert_labels + [pad_token_label_id]\n",
        "\n",
        "        # Convert tokens into their corresponding numerical IDs.\n",
        "        token_ids = tokenizer.convert_tokens_to_ids(bert_tokens)\n",
        "        # Create an attention mask that has 1s for real tokens.\n",
        "        attention_mask = [1] * len(token_ids)\n",
        "\n",
        "        # Determine how many padding tokens are needed to reach max_length.\n",
        "        padding_length = max_length - len(token_ids)\n",
        "        token_ids += [pad_token_id] * padding_length\n",
        "        attention_mask += [0] * padding_length\n",
        "        bert_labels += [pad_token_label_id] * padding_length\n",
        "\n",
        "        # Append the processed outputs to our lists.\n",
        "        input_ids.append(token_ids)\n",
        "        attention_masks.append(attention_mask)\n",
        "        labels.append(bert_labels)\n",
        "\n",
        "    # Convert the lists into PyTorch tensors and return.\n",
        "    return torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(labels)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DataLoader Creation Function\n",
        "# =============================================================================\n",
        "def create_data_loaders(train_inputs, train_masks, train_labels,\n",
        "                        val_inputs=None, val_masks=None, val_labels=None,\n",
        "                        test_inputs=None, test_masks=None, test_labels=None,\n",
        "                        batch_size=16, num_workers=4) -> Dict:\n",
        "    \"\"\"\n",
        "    Create PyTorch DataLoaders for the training, validation, and testing datasets.\n",
        "\n",
        "    This function wraps the tensor data into TensorDatasets and then DataLoaders,\n",
        "    which will handle batching, shuffling (for training), and parallel data loading.\n",
        "\n",
        "    Args:\n",
        "        train_inputs, train_masks, train_labels: Tensors for the training set.\n",
        "        val_inputs, val_masks, val_labels: Tensors for the validation set (optional).\n",
        "        test_inputs, test_masks, test_labels: Tensors for the test set (optional).\n",
        "        batch_size (int): Number of samples per batch.\n",
        "        num_workers (int): Number of subprocesses to use for data loading.\n",
        "\n",
        "    Returns:\n",
        "        Dict: A dictionary containing DataLoaders for 'train', 'validation', and 'test' splits.\n",
        "    \"\"\"\n",
        "    # Create a TensorDataset for the training data.\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,          # Shuffle data during training for randomness.\n",
        "        pin_memory=True,       # Enable faster data transfer to GPU.\n",
        "        num_workers=num_workers  # Use multiple subprocesses.\n",
        "    )\n",
        "\n",
        "    loaders = {\"train\": train_loader}\n",
        "\n",
        "    # If validation data is provided, create a corresponding DataLoader.\n",
        "    if val_inputs is not None:\n",
        "        val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "        val_loader = DataLoader(\n",
        "            val_data,\n",
        "            batch_size=batch_size,\n",
        "            pin_memory=True,\n",
        "            num_workers=num_workers\n",
        "        )\n",
        "        loaders[\"validation\"] = val_loader\n",
        "\n",
        "    # Similarly, if test data is provided, create its DataLoader.\n",
        "    if test_inputs is not None:\n",
        "        test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "        test_loader = DataLoader(\n",
        "            test_data,\n",
        "            batch_size=batch_size,\n",
        "            pin_memory=True,\n",
        "            num_workers=num_workers\n",
        "        )\n",
        "        loaders[\"test\"] = test_loader\n",
        "\n",
        "    return loaders\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Model Training Function\n",
        "# =============================================================================\n",
        "def train_model(model, data_loaders, optimizer, scheduler, device,\n",
        "                num_epochs=3, evaluation_steps=100, id_to_tag=None,\n",
        "                gradient_accumulation_steps=1):\n",
        "    \"\"\"\n",
        "    Train the BERT-based NER model and periodically evaluate it on validation data.\n",
        "\n",
        "    This training function uses gradient accumulation to simulate larger batch sizes,\n",
        "    and optionally uses mixed precision training if a GPU is available.\n",
        "\n",
        "    Args:\n",
        "        model: The BERT model for token classification.\n",
        "        data_loaders: Dictionary of DataLoaders for 'train' and 'validation' splits.\n",
        "        optimizer: Optimizer for updating model parameters.\n",
        "        scheduler: Learning rate scheduler.\n",
        "        device: Device to use for training (CPU or GPU).\n",
        "        num_epochs (int): Number of epochs to train.\n",
        "        evaluation_steps (int): How many steps between each evaluation.\n",
        "        id_to_tag: Mapping from label IDs to tag strings (for computing metrics).\n",
        "        gradient_accumulation_steps (int): Number of steps over which to accumulate gradients.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: The trained model and a history dictionary with training and validation metrics.\n",
        "    \"\"\"\n",
        "    # If a GPU is available, print its details and enable performance optimizations.\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = True  # Enable cuDNN autotuner for faster performance.\n",
        "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    model.to(device)  # Move the model to the specified device.\n",
        "    model.train()     # Set the model to training mode.\n",
        "\n",
        "    # Setup for mixed precision training if available.\n",
        "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
        "\n",
        "    # Define the loss function. Ignore any indices marked with -100 (used for padding).\n",
        "    loss_fn = CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    # Initialize a dictionary to store training history (losses and metrics).\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_f1\": [],\n",
        "        \"val_accuracy\": []\n",
        "    }\n",
        "\n",
        "    global_step = 0  # Counter for the number of gradient update steps.\n",
        "\n",
        "    # Loop over epochs.\n",
        "    for epoch in range(num_epochs):\n",
        "        total_train_loss = 0  # Accumulate loss over the epoch.\n",
        "        progress_bar = tqdm(data_loaders[\"train\"], desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Iterate through each batch in the training DataLoader.\n",
        "        for step, batch in enumerate(progress_bar):\n",
        "            # Transfer the batch to the device (GPU or CPU).\n",
        "            batch_inputs, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
        "\n",
        "            # If using mixed precision, perform forward and backward passes under autocast.\n",
        "            if scaler:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(\n",
        "                        input_ids=batch_inputs,\n",
        "                        attention_mask=batch_masks,\n",
        "                        labels=batch_labels\n",
        "                    )\n",
        "                    # Scale loss for gradient accumulation.\n",
        "                    loss = outputs.loss / gradient_accumulation_steps\n",
        "\n",
        "                # Scale the gradients and perform backward pass.\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Update model parameters after accumulating gradients.\n",
        "                if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                    scaler.unscale_(optimizer)  # Unscale gradients before clipping.\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip gradients.\n",
        "                    scaler.step(optimizer)      # Update parameters with scaled gradients.\n",
        "                    scaler.update()             # Update scaler for mixed precision.\n",
        "                    scheduler.step()            # Update learning rate.\n",
        "                    model.zero_grad()           # Clear gradients.\n",
        "                    global_step += 1            # Increment global step.\n",
        "            else:\n",
        "                # Standard training without mixed precision.\n",
        "                outputs = model(\n",
        "                    input_ids=batch_inputs,\n",
        "                    attention_mask=batch_masks,\n",
        "                    labels=batch_labels\n",
        "                )\n",
        "                loss = outputs.loss / gradient_accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "                if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip gradients.\n",
        "                    optimizer.step()       # Update model parameters.\n",
        "                    scheduler.step()       # Update the learning rate.\n",
        "                    model.zero_grad()      # Reset gradients.\n",
        "                    global_step += 1       # Increment update step count.\n",
        "\n",
        "            # Multiply loss back by accumulation steps for proper scaling.\n",
        "            total_train_loss += loss.item() * gradient_accumulation_steps\n",
        "            progress_bar.set_postfix({\"loss\": loss.item() * gradient_accumulation_steps})\n",
        "\n",
        "            # Evaluate model performance on the validation set at specified intervals.\n",
        "            if global_step > 0 and global_step % evaluation_steps == 0 and \"validation\" in data_loaders:\n",
        "                val_metrics = evaluate_model(model, data_loaders[\"validation\"], device, id_to_tag)\n",
        "                history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
        "                history[\"val_f1\"].append(val_metrics[\"f1\"])\n",
        "                history[\"val_accuracy\"].append(val_metrics[\"accuracy\"])\n",
        "                print(f\"\\nStep {global_step}: Validation Loss: {val_metrics['loss']:.4f}, \"\n",
        "                      f\"F1: {val_metrics['f1']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "                model.train()  # Return model to training mode after evaluation.\n",
        "\n",
        "        # Compute the average training loss for the epoch.\n",
        "        avg_train_loss = total_train_loss / len(data_loaders[\"train\"])\n",
        "        history[\"train_loss\"].append(avg_train_loss)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # At the end of each epoch, perform a full validation evaluation.\n",
        "        if \"validation\" in data_loaders:\n",
        "            val_metrics = evaluate_model(model, data_loaders[\"validation\"], device, id_to_tag)\n",
        "            print(f\"Validation - Loss: {val_metrics['loss']:.4f}, \"\n",
        "                  f\"F1: {val_metrics['f1']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "\n",
        "    # Return the final model and the history of training metrics.\n",
        "    return model, history\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Model Evaluation Function\n",
        "# =============================================================================\n",
        "def evaluate_model(model, data_loader, device, id_to_tag=None):\n",
        "    \"\"\"\n",
        "    Evaluate the BERT NER model on a given dataset and compute performance metrics.\n",
        "\n",
        "    The evaluation computes the average loss over the dataset, and if tag mappings\n",
        "    are provided, it calculates both token-level and entity-level precision, recall,\n",
        "    F1 score, and accuracy.\n",
        "\n",
        "    Args:\n",
        "        model: BERT model for token classification.\n",
        "        data_loader: DataLoader providing evaluation data.\n",
        "        device: Device on which evaluation is performed.\n",
        "        id_to_tag: Optional mapping from label IDs to tag strings.\n",
        "\n",
        "    Returns:\n",
        "        Dict: A dictionary containing evaluation metrics.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode.\n",
        "    total_loss = 0  # Initialize total loss.\n",
        "    loss_fn = CrossEntropyLoss(ignore_index=-100)  # Loss function ignores padded indices.\n",
        "\n",
        "    true_labels = []       # List to store true label IDs.\n",
        "    predicted_labels = []  # List to store predicted label IDs.\n",
        "\n",
        "    # Disable gradient computation during evaluation.\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            # Move batch data to device.\n",
        "            batch_inputs, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
        "\n",
        "            # Use mixed precision during evaluation if GPU is available.\n",
        "            if torch.cuda.is_available():\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(\n",
        "                        input_ids=batch_inputs,\n",
        "                        attention_mask=batch_masks\n",
        "                    )\n",
        "                    # Create a mask for active positions (where attention mask is 1).\n",
        "                    active_loss = batch_masks.view(-1) == 1\n",
        "                    active_logits = outputs.logits.view(-1, model.config.num_labels)\n",
        "                    # Set inactive positions to -100 so they are ignored in the loss.\n",
        "                    active_labels = torch.where(\n",
        "                        active_loss,\n",
        "                        batch_labels.view(-1),\n",
        "                        torch.tensor(-100).type_as(batch_labels)\n",
        "                    )\n",
        "                    loss = loss_fn(active_logits, active_labels)\n",
        "            else:\n",
        "                outputs = model(\n",
        "                    input_ids=batch_inputs,\n",
        "                    attention_mask=batch_masks\n",
        "                )\n",
        "                active_loss = batch_masks.view(-1) == 1\n",
        "                active_logits = outputs.logits.view(-1, model.config.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss,\n",
        "                    batch_labels.view(-1),\n",
        "                    torch.tensor(-100).type_as(batch_labels)\n",
        "                )\n",
        "                loss = loss_fn(active_logits, active_labels)\n",
        "\n",
        "            # Accumulate loss.\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Obtain predictions by selecting the index with maximum logit.\n",
        "            logits = outputs.logits\n",
        "            batch_preds = torch.argmax(logits, dim=2)\n",
        "\n",
        "            # Move predictions and labels to CPU for further processing.\n",
        "            labels = batch_labels.detach().cpu().numpy()\n",
        "            preds = batch_preds.detach().cpu().numpy()\n",
        "            mask = batch_masks.detach().cpu().numpy()\n",
        "\n",
        "            # Only consider tokens that are not padding (label != -100) and where mask is active.\n",
        "            for i in range(labels.shape[0]):\n",
        "                for j in range(labels.shape[1]):\n",
        "                    if labels[i, j] != -100 and mask[i, j] == 1:\n",
        "                        true_labels.append(labels[i, j])\n",
        "                        predicted_labels.append(preds[i, j])\n",
        "\n",
        "    # Compute average loss over the evaluation set.\n",
        "    metrics = {\"loss\": total_loss / len(data_loader)}\n",
        "\n",
        "    # If there are valid tokens, compute additional metrics.\n",
        "    if len(true_labels) > 0:\n",
        "        if id_to_tag:\n",
        "            # Convert numerical label IDs to their string representations.\n",
        "            id_to_tag = {int(k): v for k, v in id_to_tag.items()}\n",
        "            true_tags = [id_to_tag.get(label, \"O\") for label in true_labels]\n",
        "            pred_tags = [id_to_tag.get(label, \"O\") for label in predicted_labels]\n",
        "\n",
        "            # For entity-level evaluation, filter out non-entity tokens (\"O\").\n",
        "            entity_true = [label for label in true_tags if label != \"O\"]\n",
        "            entity_pred = [pred_tags[i] for i, label in enumerate(true_tags) if label != \"O\"]\n",
        "\n",
        "            # Compute precision, recall, and F1 score for entities.\n",
        "            entity_precision, entity_recall, entity_f1, _ = precision_recall_fscore_support(\n",
        "                entity_true, entity_pred, average='micro', zero_division=0\n",
        "            )\n",
        "            metrics[\"entity_precision\"] = entity_precision\n",
        "            metrics[\"entity_recall\"] = entity_recall\n",
        "            metrics[\"entity_f1\"] = entity_f1\n",
        "\n",
        "        # Compute token-level metrics across all tokens (including \"O\").\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            true_labels, predicted_labels, average='micro', zero_division=0\n",
        "        )\n",
        "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "        metrics.update({\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"accuracy\": accuracy\n",
        "        })\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Hyperparameter Tuning Function\n",
        "# =============================================================================\n",
        "def hyperparameter_tuning(train_inputs, train_masks, train_labels,\n",
        "                          val_inputs, val_masks, val_labels,\n",
        "                          tag_to_id, id_to_tag, device, num_epochs=3,\n",
        "                          num_workers=4):\n",
        "    \"\"\"\n",
        "    Perform hyperparameter tuning over a grid of hyperparameters.\n",
        "\n",
        "    This function tests various combinations of:\n",
        "      - Learning rate\n",
        "      - Batch size\n",
        "      - Weight decay\n",
        "      - Gradient accumulation steps\n",
        "\n",
        "    For each combination, the model is trained for a small number of epochs and evaluated\n",
        "    on the validation set. The best configuration is selected based on F1 score.\n",
        "\n",
        "    Args:\n",
        "        train_inputs, train_masks, train_labels: Training tensors.\n",
        "        val_inputs, val_masks, val_labels: Validation tensors.\n",
        "        tag_to_id: Mapping from tag strings to integer IDs.\n",
        "        id_to_tag: Mapping from integer IDs to tag strings.\n",
        "        device: Device to use for training (CPU or GPU).\n",
        "        num_epochs (int): Number of epochs for tuning (fewer epochs for quick search).\n",
        "        num_workers (int): Number of DataLoader workers.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: The best hyperparameter configuration and its corresponding evaluation metrics.\n",
        "    \"\"\"\n",
        "    # Define the grid of hyperparameters.\n",
        "    param_grid = {\n",
        "        'learning_rate': [2e-5, 3e-5, 5e-5],\n",
        "        'batch_size': [16, 32, 64],\n",
        "        'weight_decay': [0.0, 0.01],\n",
        "        'gradient_accumulation_steps': [1, 2, 4]\n",
        "    }\n",
        "\n",
        "    # Create a list of all combinations from the grid.\n",
        "    grid = list(ParameterGrid(param_grid))\n",
        "    best_f1 = 0\n",
        "    best_params = None\n",
        "    best_metrics = None\n",
        "\n",
        "    # Iterate over each combination in the hyperparameter grid.\n",
        "    for params in grid:\n",
        "        print(f\"\\n=== Testing parameters: {params} ===\\n\")\n",
        "        # Create DataLoaders using the current batch size.\n",
        "        loaders = create_data_loaders(\n",
        "            train_inputs, train_masks, train_labels,\n",
        "            val_inputs, val_masks, val_labels,\n",
        "            batch_size=params['batch_size'],\n",
        "            num_workers=num_workers\n",
        "        )\n",
        "\n",
        "        # Initialize a new model for each hyperparameter configuration.\n",
        "        model = BertForTokenClassification.from_pretrained(\n",
        "            \"bert-base-cased\",\n",
        "            num_labels=len(tag_to_id)\n",
        "        )\n",
        "\n",
        "        # Setup the optimizer with the current learning rate and weight decay.\n",
        "        optimizer = AdamW(\n",
        "            model.parameters(),\n",
        "            lr=params['learning_rate'],\n",
        "            weight_decay=params['weight_decay']\n",
        "        )\n",
        "\n",
        "        # Calculate total training steps based on batch size and gradient accumulation.\n",
        "        total_steps = len(loaders['train']) * num_epochs // params['gradient_accumulation_steps']\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=int(0.1 * total_steps),\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        # Train the model with the current hyperparameters.\n",
        "        model, _ = train_model(\n",
        "            model, loaders, optimizer, scheduler,\n",
        "            device, num_epochs=num_epochs,\n",
        "            evaluation_steps=len(loaders['train']),  # Evaluate at the end of each epoch.\n",
        "            id_to_tag=id_to_tag,\n",
        "            gradient_accumulation_steps=params['gradient_accumulation_steps']\n",
        "        )\n",
        "\n",
        "        # Evaluate the model on the validation set.\n",
        "        metrics = evaluate_model(model, loaders['validation'], device, id_to_tag)\n",
        "        if metrics['f1'] > best_f1:\n",
        "            best_f1 = metrics['f1']\n",
        "            best_params = params\n",
        "            best_metrics = metrics\n",
        "\n",
        "        print(f\"F1 score: {metrics['f1']:.4f}\\n\")\n",
        "\n",
        "    return best_params, best_metrics\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Training History Visualization Function\n",
        "# =============================================================================\n",
        "def visualize_training_history(history, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize the training history including loss curves and evaluation metrics.\n",
        "\n",
        "    Generates two subplots:\n",
        "      - Left: Training and validation loss over epochs.\n",
        "      - Right: Evaluation metrics (F1 Score and Accuracy) over evaluation steps.\n",
        "\n",
        "    Args:\n",
        "        history (dict): Dictionary containing recorded training history.\n",
        "        save_path (str): Optional file path to save the generated plot.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot loss curves.\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history[\"train_loss\"], label=\"Training Loss\", marker=\"o\")\n",
        "    if \"val_loss\" in history and history[\"val_loss\"]:\n",
        "        plt.plot(history[\"val_loss\"], label=\"Validation Loss\", marker=\"o\")\n",
        "    plt.title(\"Loss During Training\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot evaluation metrics.\n",
        "    plt.subplot(1, 2, 2)\n",
        "    if \"val_f1\" in history and history[\"val_f1\"]:\n",
        "        plt.plot(history[\"val_f1\"], label=\"F1 Score\", marker=\"o\")\n",
        "    if \"val_accuracy\" in history and history[\"val_accuracy\"]:\n",
        "        plt.plot(history[\"val_accuracy\"], label=\"Accuracy\", marker=\"o\")\n",
        "    plt.title(\"Evaluation Metrics During Training\")\n",
        "    plt.xlabel(\"Evaluation Step\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure if a path is provided; otherwise, display it.\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Main Function: Pipeline Execution\n",
        "# =============================================================================\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Execute the complete training and evaluation pipeline for the BERT-based NER model.\n",
        "\n",
        "    This includes:\n",
        "      - Device and DataLoader worker configuration.\n",
        "      - Loading preprocessed data and tag mappings.\n",
        "      - Encoding datasets using the BERT tokenizer.\n",
        "      - Optionally performing hyperparameter tuning.\n",
        "      - Final model training with the best hyperparameters.\n",
        "      - Visualization of training history.\n",
        "      - Evaluation on the test set.\n",
        "      - Saving the trained model and related artifacts.\n",
        "    \"\"\"\n",
        "    # Configure the device to use (prefer GPU if available).\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # If using GPU, clear cache and set device properties.\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_max_memory_allocated()\n",
        "        torch.cuda.set_device(0)\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    # Determine the number of DataLoader worker processes.\n",
        "    num_workers = min(os.cpu_count(), 8) if os.cpu_count() else 4\n",
        "    print(f\"Using {num_workers} dataloader workers\")\n",
        "\n",
        "    # Load preprocessed transformer data and tag mappings.\n",
        "    data_dir = \"/content/data\"\n",
        "    transformer_data, tag_mappings = load_preprocessed_data(data_dir)\n",
        "    tag_to_id = tag_mappings[\"tag_to_id\"]\n",
        "    id_to_tag = tag_mappings[\"id_to_tag\"]\n",
        "\n",
        "    # Extract text and tag sequences for training, validation, and testing.\n",
        "    train_texts = transformer_data[\"train\"][\"texts\"]\n",
        "    train_tags = transformer_data[\"train\"][\"tags\"]\n",
        "    dev_texts = transformer_data[\"dev\"][\"texts\"]\n",
        "    dev_tags = transformer_data[\"dev\"][\"tags\"]\n",
        "    test_texts = transformer_data[\"test\"][\"texts\"]\n",
        "    test_tags = transformer_data[\"test\"][\"tags\"]\n",
        "\n",
        "    print(f\"Training set: {len(train_texts)} examples\")\n",
        "    print(f\"Validation set: {len(dev_texts)} examples\")\n",
        "    print(f\"Test set: {len(test_texts)} examples\")\n",
        "\n",
        "    # Initialize the BERT tokenizer.\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "    max_length = 128  # Maximum sequence length for BERT input.\n",
        "\n",
        "    # Encode the datasets into fixed-length tensors.\n",
        "    print(\"Encoding datasets...\")\n",
        "    train_inputs, train_masks, train_labels = encode_dataset(\n",
        "        train_texts, train_tags, tokenizer, tag_to_id, max_length\n",
        "    )\n",
        "    dev_inputs, dev_masks, dev_labels = encode_dataset(\n",
        "        dev_texts, dev_tags, tokenizer, tag_to_id, max_length\n",
        "    )\n",
        "    test_inputs, test_masks, test_labels = encode_dataset(\n",
        "        test_texts, test_tags, tokenizer, tag_to_id, max_length\n",
        "    )\n",
        "    print(\"Datasets encoded successfully!\")\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # Hyperparameter Tuning (Optional)\n",
        "    # --------------------------------------------------------------------------\n",
        "    print(\"\\nStarting hyperparameter tuning...\")\n",
        "    # Use a subset of the training data for quick tuning.\n",
        "    subset_size = min(5000, len(train_inputs))\n",
        "    subset_indices = np.random.choice(len(train_inputs), subset_size, replace=False)\n",
        "\n",
        "    tuning_train_inputs = train_inputs[subset_indices]\n",
        "    tuning_train_masks = train_masks[subset_indices]\n",
        "    tuning_train_labels = train_labels[subset_indices]\n",
        "\n",
        "    # Run hyperparameter tuning over the defined grid.\n",
        "    best_params, _ = hyperparameter_tuning(\n",
        "        tuning_train_inputs, tuning_train_masks, tuning_train_labels,\n",
        "        dev_inputs, dev_masks, dev_labels,\n",
        "        tag_to_id, id_to_tag, device, num_epochs=2,  # Use fewer epochs for faster tuning.\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    print(f\"\\nBest hyperparameters: {best_params}\")\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # Final Model Training with Best Hyperparameters\n",
        "    # --------------------------------------------------------------------------\n",
        "    batch_size = best_params[\"batch_size\"]\n",
        "    data_loaders = create_data_loaders(\n",
        "        train_inputs, train_masks, train_labels,\n",
        "        dev_inputs, dev_masks, dev_labels,\n",
        "        test_inputs, test_masks, test_labels,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    print(\"\\nInitializing model...\")\n",
        "    model = BertForTokenClassification.from_pretrained(\n",
        "        \"bert-base-cased\",\n",
        "        num_labels=len(tag_to_id)\n",
        "    )\n",
        "\n",
        "    learning_rate = best_params[\"learning_rate\"]\n",
        "    weight_decay = best_params[\"weight_decay\"]\n",
        "    gradient_accumulation_steps = best_params[\"gradient_accumulation_steps\"]\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=learning_rate,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    num_epochs = 5  # Set the number of epochs for final training.\n",
        "    total_steps = len(data_loaders[\"train\"]) * num_epochs // gradient_accumulation_steps\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1 * total_steps),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining model...\")\n",
        "    model, history = train_model(\n",
        "        model, data_loaders, optimizer, scheduler,\n",
        "        device, num_epochs=num_epochs, evaluation_steps=100,\n",
        "        id_to_tag=id_to_tag,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps\n",
        "    )\n",
        "\n",
        "    # Visualize training history and save the plot.\n",
        "    print(\"Visualizing training history...\")\n",
        "    vis_dir = os.path.join(data_dir, \"visualizations\")\n",
        "    os.makedirs(vis_dir, exist_ok=True)\n",
        "    visualize_training_history(\n",
        "        history,\n",
        "        save_path=os.path.join(vis_dir, \"bert_training_history.png\")\n",
        "    )\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # Evaluate the Final Model on the Test Set\n",
        "    # --------------------------------------------------------------------------\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    test_metrics = evaluate_model(model, data_loaders[\"test\"], device, id_to_tag)\n",
        "    print(\"\\nTest Set Metrics:\")\n",
        "    print(f\"Loss: {test_metrics['loss']:.4f}\")\n",
        "    print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
        "    print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "    if \"entity_f1\" in test_metrics:\n",
        "        print(f\"\\nEntity-Level Metrics:\")\n",
        "        print(f\"Precision: {test_metrics['entity_precision']:.4f}\")\n",
        "        print(f\"Recall: {test_metrics['entity_recall']:.4f}\")\n",
        "        print(f\"F1 Score: {test_metrics['entity_f1']:.4f}\")\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # Save the Trained Model and Related Artifacts\n",
        "    # --------------------------------------------------------------------------\n",
        "    print(\"\\nSaving model...\")\n",
        "    output_dir = os.path.join(data_dir, \"models\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    model_path = os.path.join(output_dir, \"bert_ner_model\")\n",
        "    model.save_pretrained(model_path)\n",
        "    tokenizer.save_pretrained(model_path)\n",
        "\n",
        "    # Save tag mappings to JSON.\n",
        "    with open(os.path.join(model_path, \"tag_mappings.json\"), \"w\") as f:\n",
        "        json.dump(tag_mappings, f, indent=2)\n",
        "\n",
        "    # Save test metrics for later reference.\n",
        "    with open(os.path.join(model_path, \"test_metrics.json\"), \"w\") as f:\n",
        "        json.dump({k: float(v) for k, v in test_metrics.items()}, f, indent=2)\n",
        "\n",
        "    print(f\"\\nModel saved at {model_path}\")\n",
        "    print(\"\\nDone!\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Script Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
